[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is an introduction to my project on Reddit extracting data from r/WallStreetBets\nThe authors of this paper delve into the role of the social media platform Reddit in the GameStop (GME) share rally of early 2021. Specifically, they examine how discussions on the r/WallStreetBets subreddit influenced the price dynamics of GameStop. To do this, they create a custom sentiment analysis dictionary for Reddit users using the Valence Aware Dictionary and Sentiment Reasoner (VADER) sentiment analysis package and analyze a massive dataset comprising 10.8 million comments. Their analysis uncovers significant relationships between Reddit sentiments and GameStop returns at various time intervals (1-, 5-, 10-, and 30-minutes), contributing valuable insights to the expanding body of research on “meme stocks” and the impact of discussions within investment forums on intraday stock price movements. Anand and Pathak (2022)\nReddit’s WallStreetBets (WSB) community has come to prominence in light of its notable role in affecting the stock prices of what are now referred to as meme stocks. Yet very little is known about the reliability of the highly speculative investment advice disseminated on WSB. This paper analyses WSB data spanning from January 2019 to April 2021 in order to assess how successful an investment strategy relying on the community’s recommendations could have been. We detect buy and sell advice and identify the community’s most popular stocks, based on which we define a WSB portfolio. Our evaluation shows that this portfolio has grown approx. 200% over the last three years and approx. 480% over the last year, significantly outperforming the S&P500. The average short-term accuracy of buy and sell signals, in contrast, is not found to be significantly better than randomly or equally distributed buy decisions within the same time frame. However, we present a technique for estimating whether posts are proactive as opposed to reactive and show that by focusing on a subset of more promising buy signals, a trader could have made investments yielding higher returns than the broader market or the strategy of trusting all posted buy signals. Lastly, the analysis is also conducted specifically for the period before 2021 in order to factor out the effects of the GameStop hype of January 2021 - the results confirm the conclusions and suggest that the 2021 hype merely amplified pre-existing characteristics. Buz and Melo (2021)\n\nWhat sentiments and discussions on Reddit’s investing subreddits, like r/WallStreetBets, have influenced Bed Bath & Beyond stock recently?\nHow active is the Bed Bath & Beyond subreddit, and what are its members saying about the company’s stock?\nAre there any popular Reddit posts or comments that have affected the sentiment and trading activity around Bed Bath & Beyond stock?\nHave there been any notable Reddit-driven movements in Bed Bath & Beyond stock, similar to what was seen with GameStop in early 2021?\nHow do Reddit users perceive the future prospects and potential catalysts for Bed Bath & Beyond’s stock?\nAre there any memes, trends, or inside jokes related to Bed Bath & Beyond stock on Reddit that might be influencing investor sentiment?\nWhat is the sentiment analysis of Reddit comments and discussions regarding Bed Bath & Beyond stock?\nHow do Reddit discussions align with or differ from traditional financial analysis and forecasts for Bed Bath & Beyond stock?\nAre there any Reddit users or influencers known for their insights or influence on Bed Bath & Beyond stock, and what are their views?\nWhat are the pros and cons of considering Reddit sentiment and discussions when making investment decisions related to Bed Bath & Beyond stock?\n\n\n\n\n\nReferences\n\nAnand, Abhinav, and Jalaj Pathak. 2022. “The Role of Reddit in the GameStop Short Squeeze.” Economics Letters 211: 110249.\n\n\nBuz, Tolga, and Gerard de Melo. 2021. “Should You Take Investment Advice from WallStreetBets? A Data-Driven Approach.” arXiv Preprint arXiv:2105.02728."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dheeraj Oruganty",
    "section": "",
    "text": "Dheeraj Oruganty, [DO343] is a student at Georgetown University studying Masters in Data Science and Analytics. When not innovating on data platforms, Dheeraj enjoys spending time travelling and going out to explore places"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dheeraj Oruganty",
    "section": "Education",
    "text": "Education\nGeorgetown University, DC | Washington, DC\nMS in Data Science and Analytics | Aug 2023 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dheeraj Oruganty",
    "section": "Experience",
    "text": "Experience\nMachine Learning Intern - Cluzters.ai | Aug 2021 to Feb 2022"
  },
  {
    "objectID": "lab-2.1.html",
    "href": "lab-2.1.html",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Gathering text data with an API\nIMPORTANT: The lab shown here (on the website) is just an HTML version, included for reference. To download the assignment, please navigate to the Labs tab in the Share-point dropdown menu in the website’s navigation bar. The relevant assignment can be determined from the folder’s name, click on the three dots & select download to get the assignment.\nNOTE: It is recommended that you complete this .ipynb file in VS-code.\nSubmission:"
  },
  {
    "objectID": "lab-2.1.html#assignment-1",
    "href": "lab-2.1.html#assignment-1",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-1:",
    "text": "Assignment-1:\n\nRead over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='e5fb283476a34988a2f2358c58a08d0e'"
  },
  {
    "objectID": "lab-2.1.html#assignment-2",
    "href": "lab-2.1.html#assignment-2",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-2:",
    "text": "Assignment-2:\n\nUse the provided News-API code as a starting point\nSelect THREE random topics (e.g. Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Lambda handler function for API calls.\ndef lambdaHandler(topic):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n    total_requests=2\n    verbose=True\n    TOPIC = topic\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+TOPIC,\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url)\n    # print(response.url);  \n    response = response.json()\n\n    #print(json.dumps(response, indent=2))\n\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(TOPIC + '-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n    return response\n\n\n# Function to clean up strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n# Function to clean up data before conversion\ndef cleaner(response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    total_requests=2\n    verbose=True\n    print(\"AVAILABLE KEYS:\")\n    print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            #if(verbose):\n                #print(\"----------------\")\n                #print(key)\n                #print(article[key])\n                #print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    return cleaned_data\n\n\ndef dataF(cleaned_data, topic):\n    #Function to convert a cleaned data frame into a CSV file.\n    df = pd.DataFrame(cleaned_data)\n    print(df)\n    df.to_csv(str(topic) + 'cleaned.csv' ,index_label=['index','title','description'])\n\n\nif __name__ == \"__main__\":\n    #Creating dataframes for 3 different topics and storing them in CSV files.\n    topic = [\"mango\", \"DOW\" , \"Watch\"]\n    for i in topic:\n        a = lambdaHandler(topic=i)\n        b = cleaner(response=a)\n        dataF(b,i)"
  },
  {
    "objectID": "lab-2.1.html#assignment-3",
    "href": "lab-2.1.html#assignment-3",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-3:",
    "text": "Assignment-3:\n\nUse the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext='The field of machine learning is typically divided into three fundamental sub-paradigms. These include supervised learning, unsupervised learning, and reinforcement learning (RL). The discipline of reinforcement learning focuses on how intelligent agents learn to perform actions, inside a specified environment, to maximize  a cumulative reward function. Over the past several decades, there has been a push to incorporate concepts from the field of deep-learning into the agents used in RL algorithms. This has spawned the field of Deep reinforcement learning. To date, the field of deep RL has yielded stunning results in a wide range of technological applications. These include, but are not limited to, self-driving cars, autonomous game play, robotics, trading and finance, and Natural Language Processing. This course will begin with an introduction to the fundamentals of traditional, i.e. non-deep, reinforcement learning. After reviewing fundamental deep learning topics the course will transition to deep RL by incorporating artificial neural networks into the models. Topics include Markov Decision Processes, Multi-armed Bandits, Monte Carlo Methods, Temporal Difference Learning, Function Approximation, Deep Neural Networks, Actor-Critic, Deep Q-Learning, Policy Gradient Methods, and connections to Psychology and to Neuroscience.'\n\ngenerate_word_cloud(text)\n\n\n\n\n\ndef print_info(wiki_page):\n    print(\"-------------------------\")\n    print(wiki_page.title)\n    print(wiki_page.url)\n    print(wiki_page.sections)\n\n    if(verbose):\n        print(wiki_page.sections)\n        print(wiki_page.categories)\n        print(wiki_page.html)\n        print(wiki_page.images)\n        print(wiki_page.content)\n        print(wikipedia.summary(wiki_page.title, auto_suggest=False))\n        print(wiki_page.references)\n        print(wiki_page.links[0],len(page.links))\n\n#--------------------------\n# LOOP OVER COUNTRY AND TOPIC \n#--------------------------\n\ndef wordcloudGen(list3, topic_list):\n    for i in list3:\n    \n        text=''\n        #--------------------------\n        # USER INPUTS\n        #--------------------------\n        for topic in topic_list:\n            topic=topic+' in '+ i \n            print(\"topic = \",topic)\n            max_num_pages=2     #max num pages returned by wiki search\n            verbose=False\n\n            #--------------------------\n            #SEARCH FOR RELEVANT PAGES \n            #--------------------------\n            titles=wikipedia.search(topic,results=max_num_pages)\n            print(\"TITLES=\",titles)\n            \n            #--------------------------\n            #LOOP OVER TITLES\n            #--------------------------\n            num_files=0\n            for title in titles:\n                try:\n                    page = wikipedia.page(title, auto_suggest=False)\n                    print_info(page)\n                    text = text + page.content\n                    num_files+=1\n                except:\n                    print(\"SOMETHING WENT WRONG:\", title);  \n\n    generate_word_cloud(text)\n\n\nif __name__ == \"__main__\":\n    list1=['computer science']\n    list2=['Car']\n    list3=['Nike']\n    topic_list = ['System Architecture', 'Microprocessor', 'Software Engineering']\n    topic_list2 = ['horsepower', 'weight']\n    topic_list3 = ['shoes', 'basketball']\n    wordcloudGen(list1, topic_list)\n    wordcloudGen(list2, topic_list2)\n    wordcloudGen(list3, topic_list3)\n    \n\ntopic =  System Architecture in computer science\nTITLES= ['Computer architecture', 'Outline of computer science']\n-------------------------\nComputer architecture\nhttps://en.wikipedia.org/wiki/Computer_architecture\n[]\n-------------------------\nOutline of computer science\nhttps://en.wikipedia.org/wiki/Outline_of_computer_science\n[]\ntopic =  Microprocessor in computer science\nTITLES= ['Microcomputer', 'Microprocessor']\n-------------------------\nMicrocomputer\nhttps://en.wikipedia.org/wiki/Microcomputer\n[]\n-------------------------\nMicroprocessor\nhttps://en.wikipedia.org/wiki/Microprocessor\n[]\ntopic =  Software Engineering in computer science\nTITLES= ['Computer science and engineering', 'Software engineering']\n-------------------------\nComputer science and engineering\nhttps://en.wikipedia.org/wiki/Computer_science_and_engineering\n[]\n-------------------------\nSoftware engineering\nhttps://en.wikipedia.org/wiki/Software_engineering\n[]\ntopic =  horsepower in Car\nTITLES= ['List of production cars by power output', 'Horsepower']\n-------------------------\nList of production cars by power output\nhttps://en.wikipedia.org/wiki/List_of_production_cars_by_power_output\n[]\n-------------------------\nHorsepower\nhttps://en.wikipedia.org/wiki/Horsepower\n[]\ntopic =  weight in Car\nTITLES= ['Vehicle weight', 'Weight']\n-------------------------\nVehicle weight\nhttps://en.wikipedia.org/wiki/Vehicle_weight\n[]\n-------------------------\nWeight\nhttps://en.wikipedia.org/wiki/Weight\n[]\ntopic =  shoes in Nike\nTITLES= ['Nike, Inc.', 'Air Force (shoe)']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nAir Force (shoe)\nhttps://en.wikipedia.org/wiki/Air_Force_(shoe)\n[]\ntopic =  basketball in Nike\nTITLES= ['Nike, Inc.', 'Nike Elite Youth Basketball League']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nNike Elite Youth Basketball League\nhttps://en.wikipedia.org/wiki/Nike_Elite_Youth_Basketball_League\n[]"
  },
  {
    "objectID": "Classification/classification.html",
    "href": "Classification/classification.html",
    "title": "Regression",
    "section": "",
    "text": "from sklearn.metrics import roc_curve, precision_recall_curve\nfrom wordcloud import WordCloud"
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "title": "Regression",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Load your data\ndf = pd.read_csv('../data/bbbyopen.csv')  # Update the path to where your stock data is located.\n\n# Create labels based on the condition that the closing price is higher than the opening price\ndf['Label'] = (df['Close'] &gt; df['Open']).astype(int)  # 1 for price increase, 0 for decrease.\n\n# Select features and labels\nX = df[['Open', 'High', 'Low', 'Volume']]  \ny = df['Label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
  },
  {
    "objectID": "Classification/classification.html#step-2-training-the-model",
    "href": "Classification/classification.html#step-2-training-the-model",
    "title": "Regression",
    "section": "Step 2: Training the Model",
    "text": "Step 2: Training the Model\n\n# Initialize Gaussian Naive Bayes\ngnb = GaussianNB()\n\n# Train the model\ngnb.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# Make predictions\ny_pred = gnb.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Generate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(cm):\n    plt.matshow(cm, cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\nplot_confusion_matrix(cm)\n\n\n\n\nAccuracy: 0.46153846153846156\n              precision    recall  f1-score   support\n\n           0       0.50      0.86      0.63         7\n           1       0.00      0.00      0.00         6\n\n    accuracy                           0.46        13\n   macro avg       0.25      0.43      0.32        13\nweighted avg       0.27      0.46      0.34        13\n\n\n\n\n\n\n\nfpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(fpr, tpr, label='Naive Bayes Classifier')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Stock Data')\nplt.show()\n\n\n\n\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(recall, precision, label='Naive Bayes Classifier')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Stock Data')\nplt.show()\n\n\n\n\nThe metrics provided suggest that the Naive Bayes classifier is not performing well on the testing dataset. Here is an interpretation of the results, based on the provided metrics:\n\nAccuracy (0.46): This metric indicates that only about 46% of the overall predictions made by the classifier are correct. In a binary classification problem (two classes, 0 and 1), this is not much better than random guessing, which would be correct approximately 50% of the time.\nPrecision and Recall for Class 0: The precision for class 0 is 0.50, indicating that when the model predicts class 0, it is correct 50% of the time. The recall for class 0 is quite high at 0.86, showing that the model is able to identify 86% of all actual class 0 instances. However, this comes at the expense of class 1, which has neither precision nor recall scores, indicating the model did not correctly identify any class 1 instances. The F1-score for class 0, which balances precision and recall, is 0.63. This is the most robust measure among the metrics provided, but this value is still not indicative of a good predictive performance.\nClass 1 Performance: The model failed to correctly predict any instance of class 1, which is evident from the precision, recall, and F1-score all being 0. This is a clear sign that the model is heavily biased towards class 0 and is unable to generalize to class 1 instances.\nMacro Average: The macro average treats all classes equally, computing the metric independently for each class and then taking the average. The low macro average precision (0.25) and recall (0.43) further highlight the model’s poor performance across classes.\nWeighted Average: The weighted average accounts for class imbalance by weighting the metric by the number of true instances for each class. These figures (precision of 0.27 and recall of 0.46) are also low, suggesting poor performance."
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "title": "Regression",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/dheeraj/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nimport pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create a DataFrame\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')\n\n# Initialize VADER\nsia = SentimentIntensityAnalyzer()\n\n# Define a function to label sentiment based on compound score\ndef label_sentiment(row):\n    score = sia.polarity_scores(row)['compound']\n    return 'positive' if score &gt; 0.05 else 'negative' if score &lt; -0.05 else 'neutral'\n\n# Apply VADER to each title and create a new column for sentiment\ndf['sentiment'] = df['title'].apply(label_sentiment)\n\n# Now let's separate our data into training and test sets based on the 'TRAIN' column\ntrain_df = df[df['TRAIN'] == 1]\ntest_df = df[df['TRAIN'] == 0]\n\n# Preprocessing and Vectorization\nvectorizer = CountVectorizer()\n\n# Only fit the vectorizer to the training data\nX_train = vectorizer.fit_transform(train_df['title'])\ny_train = train_df['sentiment']\n\nX_test = vectorizer.transform(test_df['title'])\ny_test = test_df['sentiment']  # We won't actually have this in a real scenario\n\n# Train a Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\n# Since we don't have true labels for the test set, we'll skip the evaluation steps for now.\n# In a real-world application, you would collect the test set labels before evaluating the model.\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\n# Histogram of Sentiment Scores\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot the histogram using seaborn for better style\nsns.histplot(df['sentiment'], bins=10, kde=False)\n\n# Give the histogram a title and labels\nplt.title('Sentiment Score Distribution')\nplt.xlabel('Sentiment Score')\nplt.ylabel('Number of Text Entries')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nsns.countplot(x='sentiment', data=df, palette='viridis')\n\nplt.title('Frequency of Sentiment Labels')\nplt.xlabel('Sentiment Label')\nplt.ylabel('Frequency')\n\nplt.show()\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_94862/3616295182.py:1: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(x='sentiment', data=df, palette='viridis')"
  },
  {
    "objectID": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "href": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "title": "Regression",
    "section": "Word Cloud of Most Frequent Words in Positive Class",
    "text": "Word Cloud of Most Frequent Words in Positive Class\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n# Here, we are filtering for positive sentiment\npositive_titles = df[df['sentiment'] == 'positive']['title']\n\n# Concatenate all the positive titles into one single string\ntext = \" \".join(title for title in positive_titles)\n\n# Generate the word cloud object\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')  # Turn off the axis\nplt.show()"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming the stock data is stored in a CSV file called 'stock_data.csv'\n# If it's in a different format, you'll need to adjust the read method accordingly\ndf1 = pd.read_csv('../data/bbbyopen.csv', index_col='Date', parse_dates=True)\n\n# Basic statistics\nprint(\"Basic Statistical Details:\")\nprint(df1.describe())\n\n\nBasic Statistical Details:\n            Open       High        Low      Close  Adj Close        Volume\ncount  43.000000  43.000000  43.000000  43.000000  43.000000  4.300000e+01\nmean    9.613721  10.715116   8.859302   9.659070   9.659070  6.085124e+07\nstd     3.960792   5.189946   3.215582   3.811286   3.811286  7.683056e+07\nmin     4.940000   5.770000   4.860000   5.770000   5.770000  7.908500e+06\n25%     7.095000   7.735000   6.800000   7.180000   7.180000  1.401785e+07\n50%     8.740000   9.120000   8.350000   8.760000   8.760000  3.142170e+07\n75%    10.800000  11.740000   9.895000  10.570000  10.570000  7.872405e+07\nmax    26.940001  30.000000  22.500000  23.080000  23.080000  3.953199e+08"
  },
  {
    "objectID": "eda/eda.html#date-range",
    "href": "eda/eda.html#date-range",
    "title": "Data Exploration",
    "section": "Date Range:",
    "text": "Date Range:\nThe data ranges from August 1, 2022, to September 29, 2022, giving us nearly two months of trading data to analyze."
  },
  {
    "objectID": "eda/eda.html#price-movement",
    "href": "eda/eda.html#price-movement",
    "title": "Data Exploration",
    "section": "Price Movement:",
    "text": "Price Movement:\n\nOpening Prices:\nThe opening prices have fluctuated significantly. The stock opened at $4.94 on August 1 and reached a high opening of $26.94 on August 17, indicating a substantial price increase within the month.\n\n\nClosing Prices:\nClosing prices saw similar volatility, starting at $5.77 and spiking up to $23.08 by August 17 before dropping down to $6.19 by the end of the period.\n\n\nHighs and Lows:\nThe stock experienced considerable intraday volatility. For instance, on August 16, the stock reached a high of $28.60 but also had a low of $15.36 within the same day.\n\n\nVolume:\nTrading volume varied widely, with particularly high volumes coinciding with large price movements. For example, on August 8 and 16, where the closing prices were $11.41 and $20.65, the volumes were 122,664,300 and 395,319,900 respectively, which are significantly higher than the rest of the dates.\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Assuming `text` is a string containing the text data you want to generate a word cloud for.\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')\n\n# Assuming your DataFrame is named df and the column with text data is named 'text_column'\n\n# Combine all text from the text column into one large string\ntext = \" \".join(review for review in df.title)\n\n# Generate a word cloud image\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the generated image using matplotlib\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "eda/eda.html#findings",
    "href": "eda/eda.html#findings",
    "title": "Data Exploration",
    "section": "Findings",
    "text": "Findings\n\nKey Terms\n\nStock Focus: “GME” and “BBBY” are the most prominent terms, indicating a high volume of discussion centered around GameStop and Bed Bath & Beyond stocks.\nTrading Vocabulary: Common trading terms such as “buy,” “sell,” “share,” and “stock” are prevalent.\n\n\n\nInvestor Behavior and Sentiment\n\nCommunity Lingo: Words like “moon,” “YOLO,” “ape,” and “HODL” suggest a community-driven, speculative approach to trading, aiming for substantial short-term gains.\nMarket Tactics: Discussions are rich with strategic terms like “short,” “squeeze,” “bullish,” and “put,” showing a deep engagement with market maneuvering.\n\n\n\nTemporal Focus\n\nImmediate Action: The prominence of “today,” “tomorrow,” and “now” highlights a focus on immediate trading decisions rather than long-term investment strategies.\n\n\n\nEmotional and Personal Investment\n\nRisk and Strategy: The term “YOLO” alongside “holding” illustrates a blend of risk-taking and strategic retention in investment decisions.\n\n\n\nInfluential Figures\n\nNotable Personalities: The mention of “Ryan Cohen” points to discussions that likely reflect the impact of key individuals in the trading community."
  },
  {
    "objectID": "eda/eda.html#discussion",
    "href": "eda/eda.html#discussion",
    "title": "Data Exploration",
    "section": "Discussion",
    "text": "Discussion\nThe word cloud illustrates that the discussions are not just about trading strategies but also about community dynamics, with a marked tendency towards speculative trading. This is emblematic of a shift in investment culture, where individual investors collectively and informally share insights, strategies, and reactions to market events on digital platforms. The analysis provides a snapshot of the zeitgeist of modern retail investing, characterized by immediacy, community identity, and the influence of social media on market movements."
  },
  {
    "objectID": "eda/eda.html#conclusion",
    "href": "eda/eda.html#conclusion",
    "title": "Data Exploration",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the word cloud analysis of the text data from trading discussions has unveiled a vivid picture of the previous trading landscapes. It highlights the interplay between collective sentiment and individual decision-making, where community vernacular and strategic discourse converge. This underscores the impact of social platforms on investment practices and the evolving nature of the stock market in the digital age."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "DataG/datagather.html",
    "href": "DataG/datagather.html",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport pandas as pd\nfrom datetime import datetime, date\nimport time\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nimport os\nimport yfinance as yf\nimport praw\n\n\nmsft = yf.Ticker(\"BBY.F\")\ndata = yf.download(\"BBY.F\", start=\"2022-06-01\", end=\"2022-12-30\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nprint(data.head())\n\ndata.to_csv('BBBY_data.csv')\n\n             Open   High    Low  Close  Adj Close  Volume\nDate                                                     \n2022-06-01  8.061  8.061  7.730  7.730      7.730     380\n2022-06-02  7.797  7.811  7.797  7.811      7.811      10\n2022-06-03  7.826  7.874  7.774  7.774      7.774     450\n2022-06-06  7.539  7.539  7.539  7.539      7.539       0\n2022-06-07  7.504  7.504  7.504  7.504      7.504       0\n\n\n\nclient_id = 'hEeZW8o7s3F8rxCEIxOeXg'\nsecret_key = 'Mgzc-xjP4I78y9vHMVSezyzolmdejA'\n\nreddit1 = praw.Reddit(\n    client_id= 'hEeZW8o7s3F8rxCEIxOeXg',\n    client_secret= 'Mgzc-xjP4I78y9vHMVSezyzolmdejA',\n    password = \"pokemons\",\n    user_agent=\"test-script\",\n    username=\"dhhheeee\",\n)\nprint(reddit.user.me())\n\ndhhheeee\n\n\n\nq='BBBY' #query\nsub='WallStreetBets' #subreddit\nsort = \"top\" \nlimit = 15000\n\n\ntop_posts = reddit.subreddit(sub).search(q, sort='hot', limit=limit)\n\nprint(top_posts)\n\n&lt;praw.models.listing.generator.ListingGenerator object at 0x13f6cfcd0&gt;\n\n\n\ntotal_posts = list()\n\n\nfor post in top_posts:\n # print(vars(post)) # print all properties\n    time.sleep(1)\n    Title=post.title,\n    Score = post.score,\n    Number_Of_Comments = post.num_comments,\n    Publish_Date = post.created,\n    Link = post.permalink,\n    data_set = {\"Title\":Title[0],\"Score\":Score[0],   \"Number_Of_Comments\":Number_Of_Comments[0],\"Publish_Date\":Publish_Date[0],\"Link\":'https://www.reddit.com'+Link[0]}\n    total_posts.append(data_set)\n\n\ndf = pd.DataFrame(total_posts)\ndf.to_csv('data.csv', sep=',', index=False)\n\n\nlibrary(quantmod)\n\nLoading required package: xts\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\nbby_df &lt;- getSymbols('BBY.f', src='yahoo', auto.assign=FALSE)\n\nWarning message:\n\"BBY.f contains missing values. Some functions will not work if objects contain missing values in the middle of the series. Consider using na.omit(), na.approx(), na.fill(), etc to remove or replace them.\"\n\n\n\nchartSeries(bby_df, name=\"BBBY\", subset=\"last 24 months\", theme=chartTheme(\"white\"))\n\n\nwrite.csv(bby_df, file = \"wallstreetbets_stock.csv\", row.names = FALSE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made for DSAN 5000 Lab Assignment 1"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom io import StringIO\n\n\ndf = pd.read_csv('data.csv')\nprint(df.head())\n\ndf.columns\n\n       title score       id  \\\n0  Wash sale     0   txhzpp   \n1    Comment     1  i3lsqo4   \n2    Comment     1  i3lrk43   \n3    Comment     1  i3lmzkh   \n4    Comment     1  i3lmz13   \n\n                                                 url  comms_num       created  \\\n0  https://www.reddit.com/r/wallstreetbets/commen...        8.0  1.649236e+09   \n1                                                NaN        0.0  1.649238e+09   \n2                                                NaN        0.0  1.649237e+09   \n3                                                NaN        0.0  1.649233e+09   \n4                                                NaN        0.0  1.649233e+09   \n\n                                                body            timestamp  \n0  Hi guys I bought uvxy beginning of the year in...  2022-04-06 09:14:16  \n1                          Yes December 30 I sold it  2022-04-06 09:32:46  \n2         ![img](emote|t5_2th52|4260) Just need help  2022-04-06 09:15:56  \n3                                   ??? Alright bro.  2022-04-06 08:10:16  \n4  Elon is with more than some of the S&P 500 com...  2022-04-06 08:10:03  \n\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/3347210500.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('data.csv')\n\n\nIndex(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n       'timestamp'],\n      dtype='object')\n\n\n\nfiltered_df = df[df['title'].str.contains('BBBY', case=False, na=False)]\n\nfiltered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\nfiltered_df1 = filtered_df[filtered_df['comms_num'] &gt; 5]\n\nfiltered_df1\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2740493494.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\n\n\n\n\n\n\n\n\ntitle\nscore\nid\nurl\ncomms_num\ncreated\nbody\ntimestamp\n\n\n\n\n242\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n268\ntrrxgt\nhttps://www.reddit.com/gallery/trrxgt\n33\n1.648594e+09\nNaN\n2022-03-29 22:44:07\n\n\n285\nRyan Cohen knows what he is doing with $BBBY a...\n2221\ntr1ln5\nhttps://www.reddit.com/gallery/tr1ln5\n129\n1.648565e+09\nNaN\n2022-03-29 14:36:27\n\n\n343\nJust another gains porn post to add to the mas...\n147\ntqkb6l\nhttps://i.redd.it/cpuz69lq37q81.png\n6\n1.648504e+09\nNaN\n2022-03-28 21:49:59\n\n\n411\nBBBY 250k yolo - bend em over Ryan Cohen\n1086\ntqcqb6\nhttps://i.redd.it/gall3rdve5q81.jpg\n68\n1.648484e+09\nNaN\n2022-03-28 16:08:17\n\n\n5546\nBBBY- Why I’m bullish and you should be too.\n72\ntwjqla\nhttps://www.reddit.com/r/wallstreetbets/commen...\n75\n1.649124e+09\nDisclaimer- I have no fking clue what I’m doin...\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n421595\nBBBY can keep you warm at night\n254\n131dzxb\nhttps://i.redd.it/qfnqwecj4jwa1.png\n16\n1.682646e+09\nNaN\n2023-04-28 01:38:48\n\n\n424457\nEven in Canada the $BBBY party is over\n1135\n133txnt\nhttps://i.redd.it/4civ1miit2xa1.jpg\n115\n1.682866e+09\nNaN\n2023-04-30 14:51:39\n\n\n441966\nBBBYQ hodlers with nuked accounts watching the...\n2636\n13gmycg\nhttps://v.redd.it/koph30sg9oza1\n365\n1.683998e+09\nNaN\n2023-05-13 17:06:51\n\n\n458074\nYou can now buy all the store equipment and fi...\n430\n13v91g2\nhttps://www.reddit.com/gallery/13v91g2\n94\n1.685400e+09\nNaN\n2023-05-29 22:47:16\n\n\n542326\nBBBY is producing Degenerates\n321\n163krrj\nhttps://i.redd.it/w39r8pxdhukb1.png\n110\n1.693226e+09\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 8 columns\n\n\n\n\nfiltered_df = filtered_df1[['body', 'title', 'timestamp']]\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n242\nNaN\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n2022-03-29 22:44:07\n\n\n285\nNaN\nRyan Cohen knows what he is doing with $BBBY a...\n2022-03-29 14:36:27\n\n\n343\nNaN\nJust another gains porn post to add to the mas...\n2022-03-28 21:49:59\n\n\n411\nNaN\nBBBY 250k yolo - bend em over Ryan Cohen\n2022-03-28 16:08:17\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n\n\n421595\nNaN\nBBBY can keep you warm at night\n2023-04-28 01:38:48\n\n\n424457\nNaN\nEven in Canada the $BBBY party is over\n2023-04-30 14:51:39\n\n\n441966\nNaN\nBBBYQ hodlers with nuked accounts watching the...\n2023-05-13 17:06:51\n\n\n458074\nNaN\nYou can now buy all the store equipment and fi...\n2023-05-29 22:47:16\n\n\n542326\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\nBBBY is producing Degenerates\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 3 columns\n\n\n\n\n# Ensure that 'timestamp' column is in datetime format\nfiltered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n# Filter the data based on the date range\nstart_date = '2022-04-01'\nend_date = '2023-01-10'\nfiltered_df = filtered_df[(filtered_df['timestamp'] &gt;= start_date) & (filtered_df['timestamp'] &lt;= end_date)]\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2011051996.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n5656\nRyan Cohen tweets in code again? Is BbBY about...\nBBBY about to rocket?\n2022-04-04 04:37:21\n\n\n10641\nI’m just getting started in options and I feel...\nBBBY Short Term Estimates\n2022-04-08 18:20:48\n\n\n13821\nNaN\nShorting $BBBY let's see what happens tomorrow\n2022-04-12 23:13:21\n\n\n14750\nNaN\nBBBY YOLO - after turning 4k to 50k on BBBY sh...\n2022-04-14 23:06:44\n\n\n...\n...\n...\n...\n\n\n213329\nWent YOLO on $111k of BBBY 2024 corporate bond...\nBBBY 2024 BONDS YOLO\n2022-10-14 03:52:26\n\n\n218886\nNaN\nPsychology of Market cycles in perspective of ...\n2022-10-18 18:30:30\n\n\n244418\nI was reading his page on Wikipedia and stumbl...\nExcerpts from the class action suit against Ry...\n2022-11-09 03:58:50\n\n\n259696\nThis was his first video interview in a couple...\nRyan Cohen speaks about selling his BBBY stake 🍉\n2022-11-20 21:33:14\n\n\n296299\nNaN\nI heard you guys like gain porn (Citadel made ...\n2023-01-06 19:25:11\n\n\n\n\n2631 rows × 3 columns\n\n\n\n\nfiltered_df.to_csv('CleanedData.csv')"
  }
]