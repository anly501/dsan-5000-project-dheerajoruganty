[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is an introduction to my project on Reddit extracting data from r/WallStreetBets\nThe authors of this paper delve into the role of the social media platform Reddit in the GameStop (GME) share rally of early 2021. Specifically, they examine how discussions on the r/WallStreetBets subreddit influenced the price dynamics of GameStop. To do this, they create a custom sentiment analysis dictionary for Reddit users using the Valence Aware Dictionary and Sentiment Reasoner (VADER) sentiment analysis package and analyze a massive dataset comprising 10.8 million comments. Their analysis uncovers significant relationships between Reddit sentiments and GameStop returns at various time intervals (1-, 5-, 10-, and 30-minutes), contributing valuable insights to the expanding body of research on ‚Äúmeme stocks‚Äù and the impact of discussions within investment forums on intraday stock price movements. Anand and Pathak (2022)\nReddit‚Äôs WallStreetBets (WSB) community has come to prominence in light of its notable role in affecting the stock prices of what are now referred to as meme stocks. Yet very little is known about the reliability of the highly speculative investment advice disseminated on WSB. This paper analyses WSB data spanning from January 2019 to April 2021 in order to assess how successful an investment strategy relying on the community‚Äôs recommendations could have been. We detect buy and sell advice and identify the community‚Äôs most popular stocks, based on which we define a WSB portfolio. Our evaluation shows that this portfolio has grown approx. 200% over the last three years and approx. 480% over the last year, significantly outperforming the S&P500. The average short-term accuracy of buy and sell signals, in contrast, is not found to be significantly better than randomly or equally distributed buy decisions within the same time frame. However, we present a technique for estimating whether posts are proactive as opposed to reactive and show that by focusing on a subset of more promising buy signals, a trader could have made investments yielding higher returns than the broader market or the strategy of trusting all posted buy signals. Lastly, the analysis is also conducted specifically for the period before 2021 in order to factor out the effects of the GameStop hype of January 2021 - the results confirm the conclusions and suggest that the 2021 hype merely amplified pre-existing characteristics. Buz and Melo (2021)\n\nWhat sentiments and discussions on Reddit‚Äôs investing subreddits, like r/WallStreetBets, have influenced Bed Bath & Beyond stock recently?\nHow active is the Bed Bath & Beyond subreddit, and what are its members saying about the company‚Äôs stock?\nAre there any popular Reddit posts or comments that have affected the sentiment and trading activity around Bed Bath & Beyond stock?\nHave there been any notable Reddit-driven movements in Bed Bath & Beyond stock, similar to what was seen with GameStop in early 2021?\nHow do Reddit users perceive the future prospects and potential catalysts for Bed Bath & Beyond‚Äôs stock?\nAre there any memes, trends, or inside jokes related to Bed Bath & Beyond stock on Reddit that might be influencing investor sentiment?\nWhat is the sentiment analysis of Reddit comments and discussions regarding Bed Bath & Beyond stock?\nHow do Reddit discussions align with or differ from traditional financial analysis and forecasts for Bed Bath & Beyond stock?\nAre there any Reddit users or influencers known for their insights or influence on Bed Bath & Beyond stock, and what are their views?\nWhat are the pros and cons of considering Reddit sentiment and discussions when making investment decisions related to Bed Bath & Beyond stock?\n\n\n\n\n\nReferences\n\nAnand, Abhinav, and Jalaj Pathak. 2022. ‚ÄúThe Role of Reddit in the GameStop Short Squeeze.‚Äù Economics Letters 211: 110249.\n\n\nBuz, Tolga, and Gerard de Melo. 2021. ‚ÄúShould You Take Investment Advice from WallStreetBets? A Data-Driven Approach.‚Äù arXiv Preprint arXiv:2105.02728."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dheeraj Oruganty",
    "section": "",
    "text": "Dheeraj Oruganty, [DO343] is a student at Georgetown University studying Masters in Data Science and Analytics. When not innovating on data platforms, Dheeraj enjoys spending time travelling and going out to explore places"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dheeraj Oruganty",
    "section": "Education",
    "text": "Education\nGeorgetown University, DC | Washington, DC\nMS in Data Science and Analytics | Aug 2023 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dheeraj Oruganty",
    "section": "Experience",
    "text": "Experience\nMachine Learning Intern - Cluzters.ai | Aug 2021 to Feb 2022"
  },
  {
    "objectID": "lab-2.1.html",
    "href": "lab-2.1.html",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Gathering text data with an API\nIMPORTANT: The lab shown here (on the website) is just an HTML version, included for reference. To download the assignment, please navigate to the Labs tab in the Share-point dropdown menu in the website‚Äôs navigation bar. The relevant assignment can be determined from the folder‚Äôs name, click on the three dots & select download to get the assignment.\nNOTE: It is recommended that you complete this .ipynb file in VS-code.\nSubmission:"
  },
  {
    "objectID": "lab-2.1.html#assignment-1",
    "href": "lab-2.1.html#assignment-1",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-1:",
    "text": "Assignment-1:\n\nRead over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='e5fb283476a34988a2f2358c58a08d0e'"
  },
  {
    "objectID": "lab-2.1.html#assignment-2",
    "href": "lab-2.1.html#assignment-2",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-2:",
    "text": "Assignment-2:\n\nUse the provided News-API code as a starting point\nSelect THREE random topics (e.g.¬†Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Lambda handler function for API calls.\ndef lambdaHandler(topic):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n    total_requests=2\n    verbose=True\n    TOPIC = topic\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+TOPIC,\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url)\n    # print(response.url);  \n    response = response.json()\n\n    #print(json.dumps(response, indent=2))\n\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(TOPIC + '-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n    return response\n\n\n# Function to clean up strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[‚Äô.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n# Function to clean up data before conversion\ndef cleaner(response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    total_requests=2\n    verbose=True\n    print(\"AVAILABLE KEYS:\")\n    print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            #if(verbose):\n                #print(\"----------------\")\n                #print(key)\n                #print(article[key])\n                #print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    return cleaned_data\n\n\ndef dataF(cleaned_data, topic):\n    #Function to convert a cleaned data frame into a CSV file.\n    df = pd.DataFrame(cleaned_data)\n    print(df)\n    df.to_csv(str(topic) + 'cleaned.csv' ,index_label=['index','title','description'])\n\n\nif __name__ == \"__main__\":\n    #Creating dataframes for 3 different topics and storing them in CSV files.\n    topic = [\"mango\", \"DOW\" , \"Watch\"]\n    for i in topic:\n        a = lambdaHandler(topic=i)\n        b = cleaner(response=a)\n        dataF(b,i)"
  },
  {
    "objectID": "lab-2.1.html#assignment-3",
    "href": "lab-2.1.html#assignment-3",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-3:",
    "text": "Assignment-3:\n\nUse the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext='The field of machine learning is typically divided into three fundamental sub-paradigms. These include supervised learning, unsupervised learning, and reinforcement learning (RL). The discipline of reinforcement learning focuses on how intelligent agents learn to perform actions, inside a specified environment, to maximize  a cumulative reward function. Over the past several decades, there has been a push to incorporate concepts from the field of deep-learning into the agents used in RL algorithms. This has spawned the field of Deep reinforcement learning. To date, the field of deep RL has yielded stunning results in a wide range of technological applications. These include, but are not limited to, self-driving cars, autonomous game play, robotics, trading and finance, and Natural Language Processing. This course will begin with an introduction to the fundamentals of traditional, i.e. non-deep, reinforcement learning. After reviewing fundamental deep learning topics the course will transition to deep RL by incorporating artificial neural networks into the models. Topics include Markov Decision Processes, Multi-armed Bandits, Monte Carlo Methods, Temporal Difference Learning, Function Approximation, Deep Neural Networks, Actor-Critic, Deep Q-Learning, Policy Gradient Methods, and connections to Psychology and to Neuroscience.'\n\ngenerate_word_cloud(text)\n\n\n\n\n\ndef print_info(wiki_page):\n    print(\"-------------------------\")\n    print(wiki_page.title)\n    print(wiki_page.url)\n    print(wiki_page.sections)\n\n    if(verbose):\n        print(wiki_page.sections)\n        print(wiki_page.categories)\n        print(wiki_page.html)\n        print(wiki_page.images)\n        print(wiki_page.content)\n        print(wikipedia.summary(wiki_page.title, auto_suggest=False))\n        print(wiki_page.references)\n        print(wiki_page.links[0],len(page.links))\n\n#--------------------------\n# LOOP OVER COUNTRY AND TOPIC \n#--------------------------\n\ndef wordcloudGen(list3, topic_list):\n    for i in list3:\n    \n        text=''\n        #--------------------------\n        # USER INPUTS\n        #--------------------------\n        for topic in topic_list:\n            topic=topic+' in '+ i \n            print(\"topic = \",topic)\n            max_num_pages=2     #max num pages returned by wiki search\n            verbose=False\n\n            #--------------------------\n            #SEARCH FOR RELEVANT PAGES \n            #--------------------------\n            titles=wikipedia.search(topic,results=max_num_pages)\n            print(\"TITLES=\",titles)\n            \n            #--------------------------\n            #LOOP OVER TITLES\n            #--------------------------\n            num_files=0\n            for title in titles:\n                try:\n                    page = wikipedia.page(title, auto_suggest=False)\n                    print_info(page)\n                    text = text + page.content\n                    num_files+=1\n                except:\n                    print(\"SOMETHING WENT WRONG:\", title);  \n\n    generate_word_cloud(text)\n\n\nif __name__ == \"__main__\":\n    list1=['computer science']\n    list2=['Car']\n    list3=['Nike']\n    topic_list = ['System Architecture', 'Microprocessor', 'Software Engineering']\n    topic_list2 = ['horsepower', 'weight']\n    topic_list3 = ['shoes', 'basketball']\n    wordcloudGen(list1, topic_list)\n    wordcloudGen(list2, topic_list2)\n    wordcloudGen(list3, topic_list3)\n    \n\ntopic =  System Architecture in computer science\nTITLES= ['Computer architecture', 'Outline of computer science']\n-------------------------\nComputer architecture\nhttps://en.wikipedia.org/wiki/Computer_architecture\n[]\n-------------------------\nOutline of computer science\nhttps://en.wikipedia.org/wiki/Outline_of_computer_science\n[]\ntopic =  Microprocessor in computer science\nTITLES= ['Microcomputer', 'Microprocessor']\n-------------------------\nMicrocomputer\nhttps://en.wikipedia.org/wiki/Microcomputer\n[]\n-------------------------\nMicroprocessor\nhttps://en.wikipedia.org/wiki/Microprocessor\n[]\ntopic =  Software Engineering in computer science\nTITLES= ['Computer science and engineering', 'Software engineering']\n-------------------------\nComputer science and engineering\nhttps://en.wikipedia.org/wiki/Computer_science_and_engineering\n[]\n-------------------------\nSoftware engineering\nhttps://en.wikipedia.org/wiki/Software_engineering\n[]\ntopic =  horsepower in Car\nTITLES= ['List of production cars by power output', 'Horsepower']\n-------------------------\nList of production cars by power output\nhttps://en.wikipedia.org/wiki/List_of_production_cars_by_power_output\n[]\n-------------------------\nHorsepower\nhttps://en.wikipedia.org/wiki/Horsepower\n[]\ntopic =  weight in Car\nTITLES= ['Vehicle weight', 'Weight']\n-------------------------\nVehicle weight\nhttps://en.wikipedia.org/wiki/Vehicle_weight\n[]\n-------------------------\nWeight\nhttps://en.wikipedia.org/wiki/Weight\n[]\ntopic =  shoes in Nike\nTITLES= ['Nike, Inc.', 'Air Force (shoe)']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nAir Force (shoe)\nhttps://en.wikipedia.org/wiki/Air_Force_(shoe)\n[]\ntopic =  basketball in Nike\nTITLES= ['Nike, Inc.', 'Nike Elite Youth Basketball League']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nNike Elite Youth Basketball League\nhttps://en.wikipedia.org/wiki/Nike_Elite_Youth_Basketball_League\n[]"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for ‚Äúclustering‚Äù"
  },
  {
    "objectID": "DataG/datagather.html",
    "href": "DataG/datagather.html",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport pandas as pd\nfrom datetime import datetime, date\nimport time\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nimport os\nimport yfinance as yf\nimport praw\n\n\nmsft = yf.Ticker(\"BBY.F\")\ndata = yf.download(\"BBY.F\", start=\"2022-06-01\", end=\"2022-12-30\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nprint(data.head())\n\ndata.to_csv('BBBY_data.csv')\n\n             Open   High    Low  Close  Adj Close  Volume\nDate                                                     \n2022-06-01  8.061  8.061  7.730  7.730      7.730     380\n2022-06-02  7.797  7.811  7.797  7.811      7.811      10\n2022-06-03  7.826  7.874  7.774  7.774      7.774     450\n2022-06-06  7.539  7.539  7.539  7.539      7.539       0\n2022-06-07  7.504  7.504  7.504  7.504      7.504       0\n\n\n\nclient_id = 'hEeZW8o7s3F8rxCEIxOeXg'\nsecret_key = 'Mgzc-xjP4I78y9vHMVSezyzolmdejA'\n\nreddit1 = praw.Reddit(\n    client_id= 'hEeZW8o7s3F8rxCEIxOeXg',\n    client_secret= 'Mgzc-xjP4I78y9vHMVSezyzolmdejA',\n    password = \"pokemons\",\n    user_agent=\"test-script\",\n    username=\"dhhheeee\",\n)\nprint(reddit.user.me())\n\ndhhheeee\n\n\n\nq='BBBY' #query\nsub='WallStreetBets' #subreddit\nsort = \"top\" \nlimit = 15000\n\n\ntop_posts = reddit.subreddit(sub).search(q, sort='hot', limit=limit)\n\nprint(top_posts)\n\n&lt;praw.models.listing.generator.ListingGenerator object at 0x13f6cfcd0&gt;\n\n\n\ntotal_posts = list()\n\n\nfor post in top_posts:\n # print(vars(post)) # print all properties\n    time.sleep(1)\n    Title=post.title,\n    Score = post.score,\n    Number_Of_Comments = post.num_comments,\n    Publish_Date = post.created,\n    Link = post.permalink,\n    data_set = {\"Title\":Title[0],\"Score\":Score[0],   \"Number_Of_Comments\":Number_Of_Comments[0],\"Publish_Date\":Publish_Date[0],\"Link\":'https://www.reddit.com'+Link[0]}\n    total_posts.append(data_set)\n\n\ndf = pd.DataFrame(total_posts)\ndf.to_csv('data.csv', sep=',', index=False)\n\n\nlibrary(quantmod)\n\nLoading required package: xts\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\nbby_df &lt;- getSymbols('BBY.f', src='yahoo', auto.assign=FALSE)\n\nWarning message:\n\"BBY.f contains missing values. Some functions will not work if objects contain missing values in the middle of the series. Consider using na.omit(), na.approx(), na.fill(), etc to remove or replace them.\"\n\n\n\nchartSeries(bby_df, name=\"BBBY\", subset=\"last 24 months\", theme=chartTheme(\"white\"))\n\n\nwrite.csv(bby_df, file = \"wallstreetbets_stock.csv\", row.names = FALSE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made for DSAN 5000 Lab Assignment 1"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom io import StringIO\n\n\ndf = pd.read_csv('data.csv')\nprint(df.head())\n\ndf.columns\n\n       title score       id  \\\n0  Wash sale     0   txhzpp   \n1    Comment     1  i3lsqo4   \n2    Comment     1  i3lrk43   \n3    Comment     1  i3lmzkh   \n4    Comment     1  i3lmz13   \n\n                                                 url  comms_num       created  \\\n0  https://www.reddit.com/r/wallstreetbets/commen...        8.0  1.649236e+09   \n1                                                NaN        0.0  1.649238e+09   \n2                                                NaN        0.0  1.649237e+09   \n3                                                NaN        0.0  1.649233e+09   \n4                                                NaN        0.0  1.649233e+09   \n\n                                                body            timestamp  \n0  Hi guys I bought uvxy beginning of the year in...  2022-04-06 09:14:16  \n1                          Yes December 30 I sold it  2022-04-06 09:32:46  \n2         ![img](emote|t5_2th52|4260) Just need help  2022-04-06 09:15:56  \n3                                   ??? Alright bro.  2022-04-06 08:10:16  \n4  Elon is with more than some of the S&P 500 com...  2022-04-06 08:10:03  \n\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/3347210500.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('data.csv')\n\n\nIndex(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n       'timestamp'],\n      dtype='object')\n\n\n\nfiltered_df = df[df['title'].str.contains('BBBY', case=False, na=False)]\n\nfiltered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\nfiltered_df1 = filtered_df[filtered_df['comms_num'] &gt; 5]\n\nfiltered_df1\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2740493494.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\n\n\n\n\n\n\n\n\ntitle\nscore\nid\nurl\ncomms_num\ncreated\nbody\ntimestamp\n\n\n\n\n242\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n268\ntrrxgt\nhttps://www.reddit.com/gallery/trrxgt\n33\n1.648594e+09\nNaN\n2022-03-29 22:44:07\n\n\n285\nRyan Cohen knows what he is doing with $BBBY a...\n2221\ntr1ln5\nhttps://www.reddit.com/gallery/tr1ln5\n129\n1.648565e+09\nNaN\n2022-03-29 14:36:27\n\n\n343\nJust another gains porn post to add to the mas...\n147\ntqkb6l\nhttps://i.redd.it/cpuz69lq37q81.png\n6\n1.648504e+09\nNaN\n2022-03-28 21:49:59\n\n\n411\nBBBY 250k yolo - bend em over Ryan Cohen\n1086\ntqcqb6\nhttps://i.redd.it/gall3rdve5q81.jpg\n68\n1.648484e+09\nNaN\n2022-03-28 16:08:17\n\n\n5546\nBBBY- Why I‚Äôm bullish and you should be too.\n72\ntwjqla\nhttps://www.reddit.com/r/wallstreetbets/commen...\n75\n1.649124e+09\nDisclaimer- I have no fking clue what I‚Äôm doin...\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n421595\nBBBY can keep you warm at night\n254\n131dzxb\nhttps://i.redd.it/qfnqwecj4jwa1.png\n16\n1.682646e+09\nNaN\n2023-04-28 01:38:48\n\n\n424457\nEven in Canada the $BBBY party is over\n1135\n133txnt\nhttps://i.redd.it/4civ1miit2xa1.jpg\n115\n1.682866e+09\nNaN\n2023-04-30 14:51:39\n\n\n441966\nBBBYQ hodlers with nuked accounts watching the...\n2636\n13gmycg\nhttps://v.redd.it/koph30sg9oza1\n365\n1.683998e+09\nNaN\n2023-05-13 17:06:51\n\n\n458074\nYou can now buy all the store equipment and fi...\n430\n13v91g2\nhttps://www.reddit.com/gallery/13v91g2\n94\n1.685400e+09\nNaN\n2023-05-29 22:47:16\n\n\n542326\nBBBY is producing Degenerates\n321\n163krrj\nhttps://i.redd.it/w39r8pxdhukb1.png\n110\n1.693226e+09\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\n2023-08-28 12:28:37\n\n\n\n\n2689 rows √ó 8 columns\n\n\n\n\nfiltered_df = filtered_df1[['body', 'title', 'timestamp']]\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n242\nNaN\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n2022-03-29 22:44:07\n\n\n285\nNaN\nRyan Cohen knows what he is doing with $BBBY a...\n2022-03-29 14:36:27\n\n\n343\nNaN\nJust another gains porn post to add to the mas...\n2022-03-28 21:49:59\n\n\n411\nNaN\nBBBY 250k yolo - bend em over Ryan Cohen\n2022-03-28 16:08:17\n\n\n5546\nDisclaimer- I have no fking clue what I‚Äôm doin...\nBBBY- Why I‚Äôm bullish and you should be too.\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n\n\n421595\nNaN\nBBBY can keep you warm at night\n2023-04-28 01:38:48\n\n\n424457\nNaN\nEven in Canada the $BBBY party is over\n2023-04-30 14:51:39\n\n\n441966\nNaN\nBBBYQ hodlers with nuked accounts watching the...\n2023-05-13 17:06:51\n\n\n458074\nNaN\nYou can now buy all the store equipment and fi...\n2023-05-29 22:47:16\n\n\n542326\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\nBBBY is producing Degenerates\n2023-08-28 12:28:37\n\n\n\n\n2689 rows √ó 3 columns\n\n\n\n\n# Ensure that 'timestamp' column is in datetime format\nfiltered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n# Filter the data based on the date range\nstart_date = '2022-04-01'\nend_date = '2023-01-10'\nfiltered_df = filtered_df[(filtered_df['timestamp'] &gt;= start_date) & (filtered_df['timestamp'] &lt;= end_date)]\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2011051996.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n5546\nDisclaimer- I have no fking clue what I‚Äôm doin...\nBBBY- Why I‚Äôm bullish and you should be too.\n2022-04-05 01:54:42\n\n\n5656\nRyan Cohen tweets in code again? Is BbBY about...\nBBBY about to rocket?\n2022-04-04 04:37:21\n\n\n10641\nI‚Äôm just getting started in options and I feel...\nBBBY Short Term Estimates\n2022-04-08 18:20:48\n\n\n13821\nNaN\nShorting $BBBY let's see what happens tomorrow\n2022-04-12 23:13:21\n\n\n14750\nNaN\nBBBY YOLO - after turning 4k to 50k on BBBY sh...\n2022-04-14 23:06:44\n\n\n...\n...\n...\n...\n\n\n213329\nWent YOLO on $111k of BBBY 2024 corporate bond...\nBBBY 2024 BONDS YOLO\n2022-10-14 03:52:26\n\n\n218886\nNaN\nPsychology of Market cycles in perspective of ...\n2022-10-18 18:30:30\n\n\n244418\nI was reading his page on Wikipedia and stumbl...\nExcerpts from the class action suit against Ry...\n2022-11-09 03:58:50\n\n\n259696\nThis was his first video interview in a couple...\nRyan Cohen speaks about selling his BBBY stake üçâ\n2022-11-20 21:33:14\n\n\n296299\nNaN\nI heard you guys like gain porn (Citadel made ...\n2023-01-06 19:25:11\n\n\n\n\n2631 rows √ó 3 columns\n\n\n\n\nfiltered_df.to_csv('CleanedData.csv')"
  }
]