[
  {
    "objectID": "Dimension/dimension.html",
    "href": "Dimension/dimension.html",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "",
    "text": "Project Proposal: Dimensionality Reduction and Visualization of Stock Market Data\nProject Objectives: The primary objective of this project is to employ dimensionality reduction techniques to transform high-dimensional stock market data of Bed Bath and Beyond stock into a two-dimensional space for visualization purposes. By leveraging PCA and t-SNE, we aim to uncover underlying trends and patterns that can provide insights into the stock’s behavior. This analysis will help in identifying days with significant market movements, discovering clusters of similar trading behaviors, and potentially predicting future market trends based on historical data.\nDataset Selection: We will utilize a historical stock dataset that includes daily price information such as opening, closing, high, low, adjusted close prices, and trading volume. This dataset provides a comprehensive view of the market’s daily movements and is ideal for observing both global and local structures within the data.\nTools and Libraries:\nMethodology:"
  },
  {
    "objectID": "Dimension/dimension.html#importing-libraries",
    "href": "Dimension/dimension.html#importing-libraries",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Importing libraries",
    "text": "Importing libraries\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "Dimension/dimension.html#reading-in-the-dataframe",
    "href": "Dimension/dimension.html#reading-in-the-dataframe",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Reading in the Dataframe",
    "text": "Reading in the Dataframe\n\n\nCode\ndf = pd.read_csv('../data/bbbyopen.csv')\n\n# Remove the 'Date' and 'Adj Close' columns as they are not needed for this analysis\ndf = df.drop(['Date', 'Adj Close'], axis=1)\n\n# Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)"
  },
  {
    "objectID": "Dimension/dimension.html#performing-principal-component-analysis",
    "href": "Dimension/dimension.html#performing-principal-component-analysis",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Performing Principal Component Analysis",
    "text": "Performing Principal Component Analysis\n\n\nCode\n# PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_data)\n\n# To determine the optimal number of principal components, we look at the explained variance ratio\npca_full = PCA()\npca_full_result = pca_full.fit(scaled_data)\nexplained_variance_ratio = pca_full.explained_variance_ratio_\n\n# Plotting the cumulative explained variance to find the optimal number of components\ncumulative_variance = np.cumsum(explained_variance_ratio)\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n# Visualizing the reduced-dimensional data using the first two principal components\nplt.figure(figsize=(10, 6))\nplt.scatter(pca_result[:, 0], pca_result[:, 1])\nplt.title('PCA - First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn the above, we can see that Principal Component 1 and 2 are fairly correlated. Which makes these ideal for our dataset. Choosing 2 Principal Components is ideal in this case as the variance does not change a lot between two Principal Components and three Principal Components."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is an introduction to my project on Reddit extracting data from r/WallStreetBets\nThe authors of this paper delve into the role of the social media platform Reddit in the GameStop (GME) share rally of early 2021. Specifically, they examine how discussions on the r/WallStreetBets subreddit influenced the price dynamics of GameStop. To do this, they create a custom sentiment analysis dictionary for Reddit users using the Valence Aware Dictionary and Sentiment Reasoner (VADER) sentiment analysis package and analyze a massive dataset comprising 10.8 million comments. Their analysis uncovers significant relationships between Reddit sentiments and GameStop returns at various time intervals (1-, 5-, 10-, and 30-minutes), contributing valuable insights to the expanding body of research on “meme stocks” and the impact of discussions within investment forums on intraday stock price movements. Anand and Pathak (2022)\nReddit’s WallStreetBets (WSB) community has come to prominence in light of its notable role in affecting the stock prices of what are now referred to as meme stocks. Yet very little is known about the reliability of the highly speculative investment advice disseminated on WSB. This paper analyses WSB data spanning from January 2019 to April 2021 in order to assess how successful an investment strategy relying on the community’s recommendations could have been. We detect buy and sell advice and identify the community’s most popular stocks, based on which we define a WSB portfolio. Our evaluation shows that this portfolio has grown approx. 200% over the last three years and approx. 480% over the last year, significantly outperforming the S&P500. The average short-term accuracy of buy and sell signals, in contrast, is not found to be significantly better than randomly or equally distributed buy decisions within the same time frame. However, we present a technique for estimating whether posts are proactive as opposed to reactive and show that by focusing on a subset of more promising buy signals, a trader could have made investments yielding higher returns than the broader market or the strategy of trusting all posted buy signals. Lastly, the analysis is also conducted specifically for the period before 2021 in order to factor out the effects of the GameStop hype of January 2021 - the results confirm the conclusions and suggest that the 2021 hype merely amplified pre-existing characteristics. Buz and Melo (2021)\n\nWhat sentiments and discussions on Reddit’s investing subreddits, like r/WallStreetBets, have influenced Bed Bath & Beyond stock recently?\nHow active is the Bed Bath & Beyond subreddit, and what are its members saying about the company’s stock?\nAre there any popular Reddit posts or comments that have affected the sentiment and trading activity around Bed Bath & Beyond stock?\nHave there been any notable Reddit-driven movements in Bed Bath & Beyond stock, similar to what was seen with GameStop in early 2021?\nHow do Reddit users perceive the future prospects and potential catalysts for Bed Bath & Beyond’s stock?\nAre there any memes, trends, or inside jokes related to Bed Bath & Beyond stock on Reddit that might be influencing investor sentiment?\nWhat is the sentiment analysis of Reddit comments and discussions regarding Bed Bath & Beyond stock?\nHow do Reddit discussions align with or differ from traditional financial analysis and forecasts for Bed Bath & Beyond stock?\nAre there any Reddit users or influencers known for their insights or influence on Bed Bath & Beyond stock, and what are their views?\nWhat are the pros and cons of considering Reddit sentiment and discussions when making investment decisions related to Bed Bath & Beyond stock?\n\n\n\n\n\nReferences\n\nAnand, Abhinav, and Jalaj Pathak. 2022. “The Role of Reddit in the GameStop Short Squeeze.” Economics Letters 211: 110249.\n\n\nBuz, Tolga, and Gerard de Melo. 2021. “Should You Take Investment Advice from WallStreetBets? A Data-Driven Approach.” arXiv Preprint arXiv:2105.02728."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dheeraj Oruganty",
    "section": "",
    "text": "Dheeraj Oruganty, [DO343] is a student at Georgetown University studying Masters in Data Science and Analytics. When not innovating on data platforms, Dheeraj enjoys spending time travelling and going out to explore places"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dheeraj Oruganty",
    "section": "Education",
    "text": "Education\nGeorgetown University, DC | Washington, DC\nMS in Data Science and Analytics | Aug 2023 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dheeraj Oruganty",
    "section": "Experience",
    "text": "Experience\nMachine Learning Intern - Cluzters.ai | Aug 2021 to Feb 2022"
  },
  {
    "objectID": "lab-2.1.html",
    "href": "lab-2.1.html",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Gathering text data with an API\nIMPORTANT: The lab shown here (on the website) is just an HTML version, included for reference. To download the assignment, please navigate to the Labs tab in the Share-point dropdown menu in the website’s navigation bar. The relevant assignment can be determined from the folder’s name, click on the three dots & select download to get the assignment.\nNOTE: It is recommended that you complete this .ipynb file in VS-code.\nSubmission:"
  },
  {
    "objectID": "lab-2.1.html#assignment-1",
    "href": "lab-2.1.html#assignment-1",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-1:",
    "text": "Assignment-1:\n\nRead over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='e5fb283476a34988a2f2358c58a08d0e'"
  },
  {
    "objectID": "lab-2.1.html#assignment-2",
    "href": "lab-2.1.html#assignment-2",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-2:",
    "text": "Assignment-2:\n\nUse the provided News-API code as a starting point\nSelect THREE random topics (e.g. Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Lambda handler function for API calls.\ndef lambdaHandler(topic):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n    total_requests=2\n    verbose=True\n    TOPIC = topic\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+TOPIC,\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url)\n    # print(response.url);  \n    response = response.json()\n\n    #print(json.dumps(response, indent=2))\n\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(TOPIC + '-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n    return response\n\n\n# Function to clean up strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n# Function to clean up data before conversion\ndef cleaner(response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    total_requests=2\n    verbose=True\n    print(\"AVAILABLE KEYS:\")\n    print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            #if(verbose):\n                #print(\"----------------\")\n                #print(key)\n                #print(article[key])\n                #print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    return cleaned_data\n\n\ndef dataF(cleaned_data, topic):\n    #Function to convert a cleaned data frame into a CSV file.\n    df = pd.DataFrame(cleaned_data)\n    print(df)\n    df.to_csv(str(topic) + 'cleaned.csv' ,index_label=['index','title','description'])\n\n\nif __name__ == \"__main__\":\n    #Creating dataframes for 3 different topics and storing them in CSV files.\n    topic = [\"mango\", \"DOW\" , \"Watch\"]\n    for i in topic:\n        a = lambdaHandler(topic=i)\n        b = cleaner(response=a)\n        dataF(b,i)"
  },
  {
    "objectID": "lab-2.1.html#assignment-3",
    "href": "lab-2.1.html#assignment-3",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-3:",
    "text": "Assignment-3:\n\nUse the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext='The field of machine learning is typically divided into three fundamental sub-paradigms. These include supervised learning, unsupervised learning, and reinforcement learning (RL). The discipline of reinforcement learning focuses on how intelligent agents learn to perform actions, inside a specified environment, to maximize  a cumulative reward function. Over the past several decades, there has been a push to incorporate concepts from the field of deep-learning into the agents used in RL algorithms. This has spawned the field of Deep reinforcement learning. To date, the field of deep RL has yielded stunning results in a wide range of technological applications. These include, but are not limited to, self-driving cars, autonomous game play, robotics, trading and finance, and Natural Language Processing. This course will begin with an introduction to the fundamentals of traditional, i.e. non-deep, reinforcement learning. After reviewing fundamental deep learning topics the course will transition to deep RL by incorporating artificial neural networks into the models. Topics include Markov Decision Processes, Multi-armed Bandits, Monte Carlo Methods, Temporal Difference Learning, Function Approximation, Deep Neural Networks, Actor-Critic, Deep Q-Learning, Policy Gradient Methods, and connections to Psychology and to Neuroscience.'\n\ngenerate_word_cloud(text)\n\n\n\n\n\ndef print_info(wiki_page):\n    print(\"-------------------------\")\n    print(wiki_page.title)\n    print(wiki_page.url)\n    print(wiki_page.sections)\n\n    if(verbose):\n        print(wiki_page.sections)\n        print(wiki_page.categories)\n        print(wiki_page.html)\n        print(wiki_page.images)\n        print(wiki_page.content)\n        print(wikipedia.summary(wiki_page.title, auto_suggest=False))\n        print(wiki_page.references)\n        print(wiki_page.links[0],len(page.links))\n\n#--------------------------\n# LOOP OVER COUNTRY AND TOPIC \n#--------------------------\n\ndef wordcloudGen(list3, topic_list):\n    for i in list3:\n    \n        text=''\n        #--------------------------\n        # USER INPUTS\n        #--------------------------\n        for topic in topic_list:\n            topic=topic+' in '+ i \n            print(\"topic = \",topic)\n            max_num_pages=2     #max num pages returned by wiki search\n            verbose=False\n\n            #--------------------------\n            #SEARCH FOR RELEVANT PAGES \n            #--------------------------\n            titles=wikipedia.search(topic,results=max_num_pages)\n            print(\"TITLES=\",titles)\n            \n            #--------------------------\n            #LOOP OVER TITLES\n            #--------------------------\n            num_files=0\n            for title in titles:\n                try:\n                    page = wikipedia.page(title, auto_suggest=False)\n                    print_info(page)\n                    text = text + page.content\n                    num_files+=1\n                except:\n                    print(\"SOMETHING WENT WRONG:\", title);  \n\n    generate_word_cloud(text)\n\n\nif __name__ == \"__main__\":\n    list1=['computer science']\n    list2=['Car']\n    list3=['Nike']\n    topic_list = ['System Architecture', 'Microprocessor', 'Software Engineering']\n    topic_list2 = ['horsepower', 'weight']\n    topic_list3 = ['shoes', 'basketball']\n    wordcloudGen(list1, topic_list)\n    wordcloudGen(list2, topic_list2)\n    wordcloudGen(list3, topic_list3)\n    \n\ntopic =  System Architecture in computer science\nTITLES= ['Computer architecture', 'Outline of computer science']\n-------------------------\nComputer architecture\nhttps://en.wikipedia.org/wiki/Computer_architecture\n[]\n-------------------------\nOutline of computer science\nhttps://en.wikipedia.org/wiki/Outline_of_computer_science\n[]\ntopic =  Microprocessor in computer science\nTITLES= ['Microcomputer', 'Microprocessor']\n-------------------------\nMicrocomputer\nhttps://en.wikipedia.org/wiki/Microcomputer\n[]\n-------------------------\nMicroprocessor\nhttps://en.wikipedia.org/wiki/Microprocessor\n[]\ntopic =  Software Engineering in computer science\nTITLES= ['Computer science and engineering', 'Software engineering']\n-------------------------\nComputer science and engineering\nhttps://en.wikipedia.org/wiki/Computer_science_and_engineering\n[]\n-------------------------\nSoftware engineering\nhttps://en.wikipedia.org/wiki/Software_engineering\n[]\ntopic =  horsepower in Car\nTITLES= ['List of production cars by power output', 'Horsepower']\n-------------------------\nList of production cars by power output\nhttps://en.wikipedia.org/wiki/List_of_production_cars_by_power_output\n[]\n-------------------------\nHorsepower\nhttps://en.wikipedia.org/wiki/Horsepower\n[]\ntopic =  weight in Car\nTITLES= ['Vehicle weight', 'Weight']\n-------------------------\nVehicle weight\nhttps://en.wikipedia.org/wiki/Vehicle_weight\n[]\n-------------------------\nWeight\nhttps://en.wikipedia.org/wiki/Weight\n[]\ntopic =  shoes in Nike\nTITLES= ['Nike, Inc.', 'Air Force (shoe)']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nAir Force (shoe)\nhttps://en.wikipedia.org/wiki/Air_Force_(shoe)\n[]\ntopic =  basketball in Nike\nTITLES= ['Nike, Inc.', 'Nike Elite Youth Basketball League']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nNike Elite Youth Basketball League\nhttps://en.wikipedia.org/wiki/Nike_Elite_Youth_Basketball_League\n[]"
  },
  {
    "objectID": "Classification/classification.html",
    "href": "Classification/classification.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "from sklearn.metrics import roc_curve, precision_recall_curve\nfrom wordcloud import WordCloud"
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "title": "Naive Bayes Classifier",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Load your data\ndf = pd.read_csv('../data/bbbyopen.csv') \n\n# Create labels based on the condition that the closing price is higher than the opening price\ndf['Label'] = (df['Close'] &gt; df['Open']).astype(int)  # 1 for price increase, 0 for decrease.\n\n# Select features and labels\nX = df[['Open', 'High', 'Low', 'Volume']]  \ny = df['Label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
  },
  {
    "objectID": "Classification/classification.html#step-2-training-the-model",
    "href": "Classification/classification.html#step-2-training-the-model",
    "title": "Naive Bayes Classifier",
    "section": "Step 2: Training the Model",
    "text": "Step 2: Training the Model\n\n# Initialize Gaussian Naive Bayes\ngnb = GaussianNB()\n\n# Train the model\ngnb.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# Make predictions\ny_pred = gnb.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Generate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(cm):\n    plt.matshow(cm, cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\nplot_confusion_matrix(cm)\n\n\n\n\nAccuracy: 0.46153846153846156\n              precision    recall  f1-score   support\n\n           0       0.50      0.86      0.63         7\n           1       0.00      0.00      0.00         6\n\n    accuracy                           0.46        13\n   macro avg       0.25      0.43      0.32        13\nweighted avg       0.27      0.46      0.34        13"
  },
  {
    "objectID": "Classification/classification.html#roc-curve-for-stock-data",
    "href": "Classification/classification.html#roc-curve-for-stock-data",
    "title": "Naive Bayes Classifier",
    "section": "ROC Curve for Stock data",
    "text": "ROC Curve for Stock data\n\nfpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(fpr, tpr, label='Naive Bayes Classifier')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Stock Data')\nplt.show()"
  },
  {
    "objectID": "Classification/classification.html#precision-recall-curve-for-stock-data",
    "href": "Classification/classification.html#precision-recall-curve-for-stock-data",
    "title": "Naive Bayes Classifier",
    "section": "Precision Recall Curve for Stock Data",
    "text": "Precision Recall Curve for Stock Data\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(recall, precision, label='Naive Bayes Classifier')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Stock Data')\nplt.show()\n\n\n\n\nThe metrics provided suggest that the Naive Bayes classifier is not performing well on the testing dataset.\n\nAccuracy (0.46): This metric indicates that only about 46% of the overall predictions made by the classifier are correct. In a binary classification problem (two classes, 0 and 1), this is not much better than random guessing, which would be correct approximately 50% of the time.\nPrecision and Recall for Class 0: The precision for class 0 is 0.50, indicating that when the model predicts class 0, it is correct 50% of the time. The recall for class 0 is quite high at 0.86, showing that the model is able to identify 86% of all actual class 0 instances. However, this comes at the expense of class 1, which has neither precision nor recall scores, indicating the model did not correctly identify any class 1 instances. The F1-score for class 0, which balances precision and recall, is 0.63. This is the most robust measure among the metrics provided, but this value is still not indicative of a good predictive performance.\nClass 1 Performance: The model failed to correctly predict any instance of class 1, which is evident from the precision, recall, and F1-score all being 0. This is a clear sign that the model is heavily biased towards class 0 and is unable to generalize to class 1 instances.\nMacro Average: The macro average treats all classes equally, computing the metric independently for each class and then taking the average. The low macro average precision (0.25) and recall (0.43) further highlight the model’s poor performance across classes.\nWeighted Average: The weighted average accounts for class imbalance by weighting the metric by the number of true instances for each class. These figures (precision of 0.27 and recall of 0.46) are also low, suggesting poor performance."
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "title": "Naive Bayes Classifier",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/dheeraj/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nimport pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create a DataFrame\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')\n\n# Initialize VADER\nsia = SentimentIntensityAnalyzer()\n\n# Define a function to label sentiment based on compound score\ndef label_sentiment(row):\n    score = sia.polarity_scores(row)['compound']\n    return 'positive' if score &gt; 0.05 else 'negative' if score &lt; -0.05 else 'neutral'\n\n# Apply VADER to each title and create a new column for sentiment\ndf['sentiment'] = df['title'].apply(label_sentiment)\n\n#separating our data into training and test sets based on the 'TRAIN' column\ntrain_df = df[df['TRAIN'] == 1]\ntest_df = df[df['TRAIN'] == 0]\n\n# Preprocessing and Vectorization\nvectorizer = CountVectorizer()\n\n# Only fit the vectorizer to the training data\nX_train = vectorizer.fit_transform(train_df['title'])\ny_train = train_df['sentiment']\n\nX_test = vectorizer.transform(test_df['title'])\ny_test = test_df['sentiment'] \n\n# Train a Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()"
  },
  {
    "objectID": "Classification/classification.html#sentiment-score-distribution",
    "href": "Classification/classification.html#sentiment-score-distribution",
    "title": "Naive Bayes Classifier",
    "section": "Sentiment Score Distribution",
    "text": "Sentiment Score Distribution\n\n# Histogram of Sentiment Scores\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.histplot(df['sentiment'], bins=10, kde=False)\n\n# Give the histogram a title and labels\nplt.title('Sentiment Score Distribution')\nplt.xlabel('Sentiment Score')\nplt.ylabel('Number of Text Entries')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "Classification/classification.html#frequency-of-sentiment-labels",
    "href": "Classification/classification.html#frequency-of-sentiment-labels",
    "title": "Naive Bayes Classifier",
    "section": "Frequency of Sentiment Labels",
    "text": "Frequency of Sentiment Labels\n\nsns.countplot(x='sentiment', data=df, palette='viridis')\n\nplt.title('Frequency of Sentiment Labels')\nplt.xlabel('Sentiment Label')\nplt.ylabel('Frequency')\n\nplt.show()\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_94862/3616295182.py:1: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(x='sentiment', data=df, palette='viridis')"
  },
  {
    "objectID": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "href": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "title": "Naive Bayes Classifier",
    "section": "Word Cloud of Most Frequent Words in Positive Class",
    "text": "Word Cloud of Most Frequent Words in Positive Class\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n# Here, we are filtering for positive sentiment\npositive_titles = df[df['sentiment'] == 'positive']['title']\n\n# Concatenate all the positive titles into one single string\ntext = \" \".join(title for title in positive_titles)\n\n# Generate the word cloud object\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')  # Turn off the axis\nplt.show()"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Python:\nPython is a versatile and widely-used programming language in the field of data science due to its readability and the extensive ecosystem of data-centric libraries. For this project, the following libraries would be employed:\n\nPandas: This library is fundamental for data manipulation and analysis. It provides data structures and operations for manipulating numerical tables and time series. It is particularly well-suited for handling structured data, like the stock and text datasets we are dealing with.\nNumPy: Often used in conjunction with Pandas, NumPy adds support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. It’s essential for numerical computations.\nMatplotlib: This plotting library offers a MATLAB-like interface and is excellent for creating static, interactive, and animated visualizations in Python. It’s highly customizable and can be used to make the histograms and scatter plots that are often required in EDA.\nSeaborn: Built on top of Matplotlib, Seaborn is a statistical graphics library that provides a high-level interface for drawing attractive and informative statistical graphics. It’s particularly good for creating more complex charts like heatmaps for correlation analysis with less code.\nScikit-learn: Although primarily known for its machine learning algorithms, scikit-learn also includes various tools for data preprocessing, model selection, and evaluation metrics that are vital for the classification tasks we are undertaking.\nNatural Language Toolkit (NLTK): For text data, NLTK is a powerful Python library that provides tools for working with human language data (text). It’s useful for tasks such as sentiment analysis, tokenization, and stopwords removal.\nVADER (from NLTK): A lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media. It’s beneficial for labeling our text data based on sentiment.\nWordCloud: While not as analytical as other tools, WordCloud can be used to visualize the most prominent words in the text data, which might give insights into the overall sentiment or topics of discussion.\nJupyter Notebook or JupyterLab: For an interactive computing environment where we can combine code execution, rich text, visualizations, and other media, Jupyter Notebook or JupyterLab will be used. It’s particularly helpful for sharing the analysis process in a readable format that includes both code and commentary.\n\n\n\n\n# Import seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf1 = pd.read_csv('../data/bbbyopen.csv', index_col='Date', parse_dates=True)\n\n# Basic statistics\nprint(\"Basic Statistical Details:\")\nprint(df1.describe())\n\n\nBasic Statistical Details:\n            Open       High        Low      Close  Adj Close        Volume\ncount  43.000000  43.000000  43.000000  43.000000  43.000000  4.300000e+01\nmean    9.613721  10.715116   8.859302   9.659070   9.659070  6.085124e+07\nstd     3.960792   5.189946   3.215582   3.811286   3.811286  7.683056e+07\nmin     4.940000   5.770000   4.860000   5.770000   5.770000  7.908500e+06\n25%     7.095000   7.735000   6.800000   7.180000   7.180000  1.401785e+07\n50%     8.740000   9.120000   8.350000   8.760000   8.760000  3.142170e+07\n75%    10.800000  11.740000   9.895000  10.570000  10.570000  7.872405e+07\nmax    26.940001  30.000000  22.500000  23.080000  23.080000  3.953199e+08\n\n\n\n\n\n\n# Compute the correlation matrix\ncorr_matrix = df.corr()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n# Scatter plot of two variables with strong correlation\nsns.scatterplot(data=df, x='Open', y='Volume')\nplt.show()"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Import seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf1 = pd.read_csv('../data/bbbyopen.csv', index_col='Date', parse_dates=True)\n\n# Basic statistics\nprint(\"Basic Statistical Details:\")\nprint(df1.describe())\n\n\nBasic Statistical Details:\n            Open       High        Low      Close  Adj Close        Volume\ncount  43.000000  43.000000  43.000000  43.000000  43.000000  4.300000e+01\nmean    9.613721  10.715116   8.859302   9.659070   9.659070  6.085124e+07\nstd     3.960792   5.189946   3.215582   3.811286   3.811286  7.683056e+07\nmin     4.940000   5.770000   4.860000   5.770000   5.770000  7.908500e+06\n25%     7.095000   7.735000   6.800000   7.180000   7.180000  1.401785e+07\n50%     8.740000   9.120000   8.350000   8.760000   8.760000  3.142170e+07\n75%    10.800000  11.740000   9.895000  10.570000  10.570000  7.872405e+07\nmax    26.940001  30.000000  22.500000  23.080000  23.080000  3.953199e+08"
  },
  {
    "objectID": "eda/eda.html#correlation-matrix",
    "href": "eda/eda.html#correlation-matrix",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Compute the correlation matrix\ncorr_matrix = df.corr()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n# Scatter plot of two variables with strong correlation\nsns.scatterplot(data=df, x='Open', y='Volume')\nplt.show()"
  },
  {
    "objectID": "eda/eda.html#time-series-analysis-of-stock-data",
    "href": "eda/eda.html#time-series-analysis-of-stock-data",
    "title": "Exploratory Data Analysis",
    "section": "Time Series analysis of Stock Data",
    "text": "Time Series analysis of Stock Data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf1 = pd.read_csv('../data/bbbyopen.csv')\nprint(df1.columns)\n# Parse the 'Date' column to datetime while considering the timezone (-04:00)\ndf1['Date'] = pd.to_datetime(df1['Date'], utc=True)\n\n\ndf1.set_index('Date', inplace=True)\n\n# Convert the index to the desired timezone (the data has -04:00 which corresponds to US Eastern time)\ndf1.index = df1.index.tz_convert('US/Eastern')\n\n# Plotting 'Close' price\nplt.figure(figsize=(14, 7))\nplt.plot(df1.index, df1['Close'], label='Close Price', marker='o')\n\n# Adding titles and labels\nplt.title('Stock Close Price Time Series')\nplt.xlabel('Date')\nplt.ylabel('Close Price in $')\nplt.legend()\n\n# Improving design\nplt.grid(True)\nplt.tight_layout()\n\n# Rotate date labels\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\nIndex(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
  },
  {
    "objectID": "eda/eda.html#date-range",
    "href": "eda/eda.html#date-range",
    "title": "Exploratory Data Analysis",
    "section": "Date Range:",
    "text": "Date Range:\nThe data ranges from August 1, 2022, to September 29, 2022, giving us nearly two months of trading data to analyze."
  },
  {
    "objectID": "eda/eda.html#price-movement",
    "href": "eda/eda.html#price-movement",
    "title": "Exploratory Data Analysis",
    "section": "Price Movement:",
    "text": "Price Movement:\n\nOpening Prices:\nThe opening prices have fluctuated significantly. The stock opened at $4.94 on August 1 and reached a high opening of $26.94 on August 17, indicating a substantial price increase within the month.\n\n\nClosing Prices:\nClosing prices saw similar volatility, starting at $5.77 and spiking up to $23.08 by August 17 before dropping down to $6.19 by the end of the period.\n\n\nHighs and Lows:\nThe stock experienced considerable intraday volatility. For instance, on August 16, the stock reached a high of $28.60 but also had a low of $15.36 within the same day.\n\n\nVolume:\nTrading volume varied widely, with particularly high volumes coinciding with large price movements. For example, on August 8 and 16, where the closing prices were $11.41 and $20.65, the volumes were 122,664,300 and 395,319,900 respectively, which are significantly higher than the rest of the dates."
  },
  {
    "objectID": "eda/eda.html#findings",
    "href": "eda/eda.html#findings",
    "title": "Exploratory Data Analysis",
    "section": "Findings",
    "text": "Findings\n\nKey Terms\n\nStock Focus: “GME” and “BBBY” are the most prominent terms, indicating a high volume of discussion centered around GameStop and Bed Bath & Beyond stocks.\nTrading Vocabulary: Common trading terms such as “buy,” “sell,” “share,” and “stock” are prevalent.\n\n\n\nInvestor Behavior and Sentiment\n\nCommunity Lingo: Words like “moon,” “YOLO,” “ape,” and “HODL” suggest a community-driven, speculative approach to trading, aiming for substantial short-term gains.\nMarket Tactics: Discussions are rich with strategic terms like “short,” “squeeze,” “bullish,” and “put,” showing a deep engagement with market maneuvering.\n\n\n\nTemporal Focus\n\nImmediate Action: The prominence of “today,” “tomorrow,” and “now” highlights a focus on immediate trading decisions rather than long-term investment strategies.\n\n\n\nEmotional and Personal Investment\n\nRisk and Strategy: The term “YOLO” alongside “holding” illustrates a blend of risk-taking and strategic retention in investment decisions.\n\n\n\nInfluential Figures\n\nNotable Personalities: The mention of “Ryan Cohen” points to discussions that likely reflect the impact of key individuals in the trading community."
  },
  {
    "objectID": "eda/eda.html#discussion",
    "href": "eda/eda.html#discussion",
    "title": "Exploratory Data Analysis",
    "section": "Discussion",
    "text": "Discussion\nThe word cloud illustrates that the discussions are not just about trading strategies but also about community dynamics, with a marked tendency towards speculative trading. This is emblematic of a shift in investment culture, where individual investors collectively and informally share insights, strategies, and reactions to market events on digital platforms. The analysis provides a snapshot of the zeitgeist of modern retail investing, characterized by immediacy, community identity, and the influence of social media on market movements."
  },
  {
    "objectID": "eda/eda.html#conclusion",
    "href": "eda/eda.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the word cloud analysis of the text data from trading discussions has unveiled a vivid picture of the previous trading landscapes. It highlights the interplay between collective sentiment and individual decision-making, where community vernacular and strategic discourse converge. This underscores the impact of social platforms on investment practices and the evolving nature of the stock market in the digital age."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction\nIn my project, I’m delving into my dataset gathered from Reddit, which comprises titles from posts on the Reddit WallStreetBets (WSB) forum which are related to Bed Bath and Beyond. My goal with the clustering analysis is to sift through these titles for patterns and themes that reveal the collective mood and recurring topics of discussion. I want to see which sentiments dominate the conversation and understand the nature of the discourse—whether it’s generally bullish or bearish, and if it’s suggesting specific trading actions. This endeavor is not just about mapping out the frequency of mentions but also about gauging the sway this online community has over market sentiment and stock movements.\nBy applying clustering to the post titles, I’m looking to categorize them into clusters that might correspond to different sentiments, investment advice, or highlight the most talked-about ‘meme stocks’. I aim to ascertain the reliability of the sentiments expressed and consider the feasibility of crafting an investment strategy based on the collective insights of the WSB community. Additionally, I’m curious to compare the activity before and after the Bed Bath & Beyond incident to determine if that was a standalone event or a magnification of a pre-existing pattern within the subreddit’s interactions.\nK-Means Clustering\nK-Means clustering is like organizing a university’s clubs fair. We want to set up booths so that students with similar interests gather around them. We decide on a number of booths (say, for music, sports, tech, literature, and volunteering). We place a sign for each in the middle of the field and watch students flock to the one they’re interested in. After the initial rush, we see that the tech group is crowded while the literature group is sparse. So, we move the signs to the actual centers of the student clusters. We repeat this process, adjusting the locations of the signs until the groups are well sorted, and the signs don’t need to be moved anymore because they accurately represent the center of student interests.\nTo figure out the best number of booths, we might use the “elbow method.” Imagine plotting the number of booths against how well they represent student interests. There’s a point where adding more booths doesn’t significantly improve the grouping - that’s our “elbow,” the sweet spot for the number of clusters.\nDBSCAN\nDBSCAN is akin to observing student interactions during a free period in a common area. Instead of predetermined booths, we watch natural groups form. DBSCAN looks for where students are densely gathered, considering a group as a cluster if there are enough students within a conversation circle. Those sitting alone far from groups, like students studying solo, are left as outliers and not forced into a cluster.\nThis method shines when you don’t know how many groups there might be and when groups vary in size. DBSCAN doesn’t require a method like the “elbow,” as it determines clusters based on student density in different areas, recognizing natural gatherings.\nHierarchical Clustering\nHierarchical clustering is comparable to creating a network of student mentorship within a large campus. Each student starts as their own entity. Slowly, we pair up students with the most in common into buddy systems. These pairs then join with others to form groups, and the groups join to form larger communities. This continues until there’s a large hierarchy representing the entire student body. We can then decide at what level we establish divisions, creating separate communities for different academic or extracurricular interests.\nTo decide where to draw these divisions, we might use a silhouette score. This score helps us ensure that each student within a group is matched well with their peers, and distinct from students in other groups, giving a sense of belonging within the community while maintaining diversity.\nIn a university setting, each clustering method offers a distinct approach to fostering student communities — whether it’s through structured organization, recognizing organic groupings, or building connections in a stepwise fashion.\n\n\nKmeans and DBSCAN\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import DBSCAN, MeanShift, KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\n\nElbow Method for K-Means clustering\n\ndf = pd.read_csv('../data/CleanedData.csv')\n\n\nvectorizer = TfidfVectorizer(stop_words='english')\ntext_features = vectorizer.fit_transform(df['title'])\n\n# No need to scale the features as TF-IDF produces normalized vectors\npca = PCA(n_components=2)\nreduced_features = pca.fit_transform(text_features.toarray())\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan_labels = dbscan.fit_predict(text_features)\n\n# Apply KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans_labels = kmeans.fit_predict(text_features)\n\n# Apply Mean Shift clustering\n# meanshift = MeanShift(bandwidth=None)  # Bandwidth will be estimated by the algorithm\n# meanshift_labels = meanshift.fit_predict(text_features)\n\ndbscan_labels, kmeans_labels\n\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\n\n# DBSCAN\nscatter = ax[0].scatter(reduced_features[:, 0], reduced_features[:, 1], c=dbscan_labels, cmap='viridis', marker='o')\nax[0].set_title('DBSCAN Clustering')\nax[0].set_xlabel('PCA Component 1')\nax[0].set_ylabel('PCA Component 2')\nlegend1 = ax[0].legend(*scatter.legend_elements(), title=\"Clusters\")\nax[0].add_artist(legend1)\n\n# KMeans\nscatter = ax[1].scatter(reduced_features[:, 0], reduced_features[:, 1], c=kmeans_labels, cmap='viridis', marker='o')\nax[1].set_title('KMeans Clustering')\nax[1].set_xlabel('PCA Component 1')\nax[1].set_ylabel('PCA Component 2')\nlegend2 = ax[1].legend(*scatter.legend_elements(), title=\"Clusters\")\nax[1].add_artist(legend2)\n\nplt.show()\n\n/Users/dheeraj/miniconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\n\n\n\nn_clusters = 6\nmodel = KMeans(n_clusters=n_clusters, random_state=42)\nmodel.fit(text_features)\n\n# Extracting features and words for word clouds\nfeatures = vectorizer.get_feature_names_out()\nclusters = model.labels_\n\n# Creating a dictionary to hold words for each cluster\nclustered_words = defaultdict(list)\nfor i, label in enumerate(clusters):\n    for feature, value in zip(features, text_features.toarray()[i]):\n        if value &gt; 0:  # adding only words present in the document\n            clustered_words[label].append(feature)\n\n# Generating word clouds for each cluster\nfig, axes = plt.subplots(1, n_clusters, figsize=(15, 5))\nfor i in range(n_clusters):\n    wordcloud = WordCloud(width=400, height=400, background_color='white').generate(' '.join(clustered_words[i]))\n    axes[i].imshow(wordcloud, interpolation='bilinear')\n    axes[i].axis('off')\n    axes[i].set_title(f'Cluster {i+1}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nDBSCAN Clustering Insights:\n\nThe data contains several dense regions with varying densities, suggesting multiple subgroups or patterns within the dataset.\nThe presence of noise (points labeled with “-1”) indicates outliers in the data that do not fit well within any cluster. This can be crucial for anomaly detection or to identify unique instances in the dataset.\nThe clusters have different shapes and sizes, which could imply that the underlying phenomena or behaviors that generated the data are complex and non-uniform.\nThe ability of DBSCAN to find these arbitrary shaped clusters could be essential for applications where the cluster shape is irregular, such as geographic data clustering or identifying regions of interest in images. # KMeans Clustering Insights:\nThere are three distinct groups in the data, which might represent three different categories or types within the dataset.\nThe tightness of the clusters could suggest that the data points within each cluster are similar to each other, which can be useful for segmenting the data into clear, distinct groups.\n\n\n\nHierarchical Clustering\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(df['title'])\n\ncosine_matrix = cosine_similarity(X)\n\n# Compute the cosine similarity matrix\ncosine_matrix_dense = cosine_matrix.astype(float)\n\n# Perform hierarchical clustering using the dense cosine similarity matrix\nZ = linkage(cosine_matrix_dense, method='average', metric='euclidean')\n\n\n\nfrom scipy.spatial.distance import squareform\n\n# Compute the cosine dissimilarity matrix (1 - similarity)\ncosine_dissimilarity_matrix = 1 - cosine_matrix\n\n# Ensure the dissimilarity matrix is non-negative\ncosine_dissimilarity_matrix[cosine_dissimilarity_matrix &lt; 0] = 0\n\n# Convert the cosine dissimilarity matrix to a condensed form\ncondensed_matrix = squareform(cosine_dissimilarity_matrix, checks=False)\n\n# Perform hierarchical clustering using the condensed cosine dissimilarity matrix\nZ = linkage(condensed_matrix, method='average')\n\n# Plot the dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(Z, labels=[f\"Document {i+1}\" for i in range(len(df))])\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Text')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "DataG/datagather.html",
    "href": "DataG/datagather.html",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport pandas as pd\nfrom datetime import datetime, date\nimport time\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nimport os\nimport yfinance as yf\nimport praw\n\n\nmsft = yf.Ticker(\"BBY.F\")\ndata = yf.download(\"BBY.F\", start=\"2022-06-01\", end=\"2022-12-30\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nprint(data.head())\n\ndata.to_csv('BBBY_data.csv')\n\n             Open   High    Low  Close  Adj Close  Volume\nDate                                                     \n2022-06-01  8.061  8.061  7.730  7.730      7.730     380\n2022-06-02  7.797  7.811  7.797  7.811      7.811      10\n2022-06-03  7.826  7.874  7.774  7.774      7.774     450\n2022-06-06  7.539  7.539  7.539  7.539      7.539       0\n2022-06-07  7.504  7.504  7.504  7.504      7.504       0\n\n\n\nclient_id = 'hEeZW8o7s3F8rxCEIxOeXg'\nsecret_key = 'Mgzc-xjP4I78y9vHMVSezyzolmdejA'\n\nreddit1 = praw.Reddit(\n    client_id= 'hEeZW8o7s3F8rxCEIxOeXg',\n    client_secret= 'Mgzc-xjP4I78y9vHMVSezyzolmdejA',\n    password = \"pokemons\",\n    user_agent=\"test-script\",\n    username=\"dhhheeee\",\n)\nprint(reddit.user.me())\n\ndhhheeee\n\n\n\nq='BBBY' #query\nsub='WallStreetBets' #subreddit\nsort = \"top\" \nlimit = 15000\n\n\ntop_posts = reddit.subreddit(sub).search(q, sort='hot', limit=limit)\n\nprint(top_posts)\n\n&lt;praw.models.listing.generator.ListingGenerator object at 0x13f6cfcd0&gt;\n\n\n\ntotal_posts = list()\n\n\nfor post in top_posts:\n # print(vars(post)) # print all properties\n    time.sleep(1)\n    Title=post.title,\n    Score = post.score,\n    Number_Of_Comments = post.num_comments,\n    Publish_Date = post.created,\n    Link = post.permalink,\n    data_set = {\"Title\":Title[0],\"Score\":Score[0],   \"Number_Of_Comments\":Number_Of_Comments[0],\"Publish_Date\":Publish_Date[0],\"Link\":'https://www.reddit.com'+Link[0]}\n    total_posts.append(data_set)\n\n\ndf = pd.DataFrame(total_posts)\ndf.to_csv('data.csv', sep=',', index=False)\n\n\nlibrary(quantmod)\n\nLoading required package: xts\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\nbby_df &lt;- getSymbols('BBY.f', src='yahoo', auto.assign=FALSE)\n\nWarning message:\n\"BBY.f contains missing values. Some functions will not work if objects contain missing values in the middle of the series. Consider using na.omit(), na.approx(), na.fill(), etc to remove or replace them.\"\n\n\n\nchartSeries(bby_df, name=\"BBBY\", subset=\"last 24 months\", theme=chartTheme(\"white\"))\n\n\nwrite.csv(bby_df, file = \"wallstreetbets_stock.csv\", row.names = FALSE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made for DSAN 5000 Lab Assignment 1"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom io import StringIO\n\n\ndf = pd.read_csv('data.csv')\nprint(df.head())\n\ndf.columns\n\n       title score       id  \\\n0  Wash sale     0   txhzpp   \n1    Comment     1  i3lsqo4   \n2    Comment     1  i3lrk43   \n3    Comment     1  i3lmzkh   \n4    Comment     1  i3lmz13   \n\n                                                 url  comms_num       created  \\\n0  https://www.reddit.com/r/wallstreetbets/commen...        8.0  1.649236e+09   \n1                                                NaN        0.0  1.649238e+09   \n2                                                NaN        0.0  1.649237e+09   \n3                                                NaN        0.0  1.649233e+09   \n4                                                NaN        0.0  1.649233e+09   \n\n                                                body            timestamp  \n0  Hi guys I bought uvxy beginning of the year in...  2022-04-06 09:14:16  \n1                          Yes December 30 I sold it  2022-04-06 09:32:46  \n2         ![img](emote|t5_2th52|4260) Just need help  2022-04-06 09:15:56  \n3                                   ??? Alright bro.  2022-04-06 08:10:16  \n4  Elon is with more than some of the S&P 500 com...  2022-04-06 08:10:03  \n\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/3347210500.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('data.csv')\n\n\nIndex(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n       'timestamp'],\n      dtype='object')\n\n\n\nfiltered_df = df[df['title'].str.contains('BBBY', case=False, na=False)]\n\nfiltered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\nfiltered_df1 = filtered_df[filtered_df['comms_num'] &gt; 5]\n\nfiltered_df1\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2740493494.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\n\n\n\n\n\n\n\n\ntitle\nscore\nid\nurl\ncomms_num\ncreated\nbody\ntimestamp\n\n\n\n\n242\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n268\ntrrxgt\nhttps://www.reddit.com/gallery/trrxgt\n33\n1.648594e+09\nNaN\n2022-03-29 22:44:07\n\n\n285\nRyan Cohen knows what he is doing with $BBBY a...\n2221\ntr1ln5\nhttps://www.reddit.com/gallery/tr1ln5\n129\n1.648565e+09\nNaN\n2022-03-29 14:36:27\n\n\n343\nJust another gains porn post to add to the mas...\n147\ntqkb6l\nhttps://i.redd.it/cpuz69lq37q81.png\n6\n1.648504e+09\nNaN\n2022-03-28 21:49:59\n\n\n411\nBBBY 250k yolo - bend em over Ryan Cohen\n1086\ntqcqb6\nhttps://i.redd.it/gall3rdve5q81.jpg\n68\n1.648484e+09\nNaN\n2022-03-28 16:08:17\n\n\n5546\nBBBY- Why I’m bullish and you should be too.\n72\ntwjqla\nhttps://www.reddit.com/r/wallstreetbets/commen...\n75\n1.649124e+09\nDisclaimer- I have no fking clue what I’m doin...\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n421595\nBBBY can keep you warm at night\n254\n131dzxb\nhttps://i.redd.it/qfnqwecj4jwa1.png\n16\n1.682646e+09\nNaN\n2023-04-28 01:38:48\n\n\n424457\nEven in Canada the $BBBY party is over\n1135\n133txnt\nhttps://i.redd.it/4civ1miit2xa1.jpg\n115\n1.682866e+09\nNaN\n2023-04-30 14:51:39\n\n\n441966\nBBBYQ hodlers with nuked accounts watching the...\n2636\n13gmycg\nhttps://v.redd.it/koph30sg9oza1\n365\n1.683998e+09\nNaN\n2023-05-13 17:06:51\n\n\n458074\nYou can now buy all the store equipment and fi...\n430\n13v91g2\nhttps://www.reddit.com/gallery/13v91g2\n94\n1.685400e+09\nNaN\n2023-05-29 22:47:16\n\n\n542326\nBBBY is producing Degenerates\n321\n163krrj\nhttps://i.redd.it/w39r8pxdhukb1.png\n110\n1.693226e+09\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 8 columns\n\n\n\n\nfiltered_df = filtered_df1[['body', 'title', 'timestamp']]\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n242\nNaN\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n2022-03-29 22:44:07\n\n\n285\nNaN\nRyan Cohen knows what he is doing with $BBBY a...\n2022-03-29 14:36:27\n\n\n343\nNaN\nJust another gains porn post to add to the mas...\n2022-03-28 21:49:59\n\n\n411\nNaN\nBBBY 250k yolo - bend em over Ryan Cohen\n2022-03-28 16:08:17\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n\n\n421595\nNaN\nBBBY can keep you warm at night\n2023-04-28 01:38:48\n\n\n424457\nNaN\nEven in Canada the $BBBY party is over\n2023-04-30 14:51:39\n\n\n441966\nNaN\nBBBYQ hodlers with nuked accounts watching the...\n2023-05-13 17:06:51\n\n\n458074\nNaN\nYou can now buy all the store equipment and fi...\n2023-05-29 22:47:16\n\n\n542326\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\nBBBY is producing Degenerates\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 3 columns\n\n\n\n\n# Ensure that 'timestamp' column is in datetime format\nfiltered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n# Filter the data based on the date range\nstart_date = '2022-04-01'\nend_date = '2023-01-10'\nfiltered_df = filtered_df[(filtered_df['timestamp'] &gt;= start_date) & (filtered_df['timestamp'] &lt;= end_date)]\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2011051996.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n5656\nRyan Cohen tweets in code again? Is BbBY about...\nBBBY about to rocket?\n2022-04-04 04:37:21\n\n\n10641\nI’m just getting started in options and I feel...\nBBBY Short Term Estimates\n2022-04-08 18:20:48\n\n\n13821\nNaN\nShorting $BBBY let's see what happens tomorrow\n2022-04-12 23:13:21\n\n\n14750\nNaN\nBBBY YOLO - after turning 4k to 50k on BBBY sh...\n2022-04-14 23:06:44\n\n\n...\n...\n...\n...\n\n\n213329\nWent YOLO on $111k of BBBY 2024 corporate bond...\nBBBY 2024 BONDS YOLO\n2022-10-14 03:52:26\n\n\n218886\nNaN\nPsychology of Market cycles in perspective of ...\n2022-10-18 18:30:30\n\n\n244418\nI was reading his page on Wikipedia and stumbl...\nExcerpts from the class action suit against Ry...\n2022-11-09 03:58:50\n\n\n259696\nThis was his first video interview in a couple...\nRyan Cohen speaks about selling his BBBY stake 🍉\n2022-11-20 21:33:14\n\n\n296299\nNaN\nI heard you guys like gain porn (Citadel made ...\n2023-01-06 19:25:11\n\n\n\n\n2631 rows × 3 columns\n\n\n\n\nfiltered_df.to_csv('CleanedData.csv')"
  }
]