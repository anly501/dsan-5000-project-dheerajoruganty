[
  {
    "objectID": "Dimension/dimension.html",
    "href": "Dimension/dimension.html",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "",
    "text": "Project Proposal: Dimensionality Reduction and Visualization of Stock Market Data\nProject Objectives: The primary objective of this project is to employ dimensionality reduction techniques to transform high-dimensional stock market data of Bed Bath and Beyond stock into a two-dimensional space for visualization purposes. By leveraging PCA and t-SNE, we aim to uncover underlying trends and patterns that can provide insights into the stock’s behavior. This analysis will help in identifying days with significant market movements, discovering clusters of similar trading behaviors, and potentially predicting future market trends based on historical data.\nDataset Selection: We will utilize a historical stock dataset that includes daily price information such as opening, closing, high, low, adjusted close prices, and trading volume. This dataset provides a comprehensive view of the market’s daily movements and is ideal for observing both global and local structures within the data.\nTools and Libraries:\nMethodology:"
  },
  {
    "objectID": "Dimension/dimension.html#importing-libraries",
    "href": "Dimension/dimension.html#importing-libraries",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Importing libraries",
    "text": "Importing libraries\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "Dimension/dimension.html#reading-in-the-dataframe",
    "href": "Dimension/dimension.html#reading-in-the-dataframe",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Reading in the Dataframe",
    "text": "Reading in the Dataframe\n\n\nCode\ndf = pd.read_csv('../data/bbbyopen.csv')\n\n# Remove the 'Date' and 'Adj Close' columns as they are not needed for this analysis\ndf = df.drop(['Date', 'Adj Close'], axis=1)\n\n# Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)"
  },
  {
    "objectID": "Dimension/dimension.html#performing-principal-component-analysis",
    "href": "Dimension/dimension.html#performing-principal-component-analysis",
    "title": "Dimensionality Reduction and Principal Component Analysis",
    "section": "Performing Principal Component Analysis",
    "text": "Performing Principal Component Analysis\n\n\nCode\n# PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_data)\n\n# To determine the optimal number of principal components, we look at the explained variance ratio\npca_full = PCA()\npca_full_result = pca_full.fit(scaled_data)\nexplained_variance_ratio = pca_full.explained_variance_ratio_\n\n# Plotting the cumulative explained variance to find the optimal number of components\ncumulative_variance = np.cumsum(explained_variance_ratio)\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n# Visualizing the reduced-dimensional data using the first two principal components\nplt.figure(figsize=(10, 6))\nplt.scatter(pca_result[:, 0], pca_result[:, 1])\nplt.title('PCA - First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn the above, we can see that Principal Component 1 and 2 are fairly correlated. Which makes these ideal for our dataset. Choosing 2 Principal Components is ideal in this case as the variance does not change a lot between two Principal Components and three Principal Components."
  },
  {
    "objectID": "ARM/arm.html",
    "href": "ARM/arm.html",
    "title": "Association Rule Mining",
    "section": "",
    "text": "Association rule mining (ARM) is a data analysis technique for uncovering relationships among variables in large datasets. Essentially, it operates on the principle of identifying frequent patterns, which are combinations of items that appear together in the dataset with regularity.\nThe method revolves around a key data mining algorithm known as the Frequent Pattern Mining (FPM) algorithm. FPM’s core objective is to reveal correlations and associations between items that may not be immediately obvious. This technique surfaces patterns that indicate if the presence of one set of items is related to the presence of another set.\nThese patterns are typically expressed through association rules, a form of rule-based machine learning. Association rules are useful not only for uncovering anomalies and regularities within data but also for a variety of other applications. These applications range widely, from analysis in retail and marketing, such as basket analysis and cross-marketing strategies, to more technical realms like software bug tracking.\nThe discovery of frequent itemsets using algorithms like Apriori is integral to many data mining initiatives. These itemsets form the foundation for more complex analysis, such as sequence discovery, interesting pattern recognition, and, most notably, the mining of association rules.\nA practical example of association rules in action is their use in understanding consumer behavior in retail settings. Such rules might reveal the frequency and conditions under which certain products are purchased together, offering valuable insights for marketing strategies and inventory management."
  },
  {
    "objectID": "ARM/arm.html#association-rule-mining",
    "href": "ARM/arm.html#association-rule-mining",
    "title": "Association Rule Mining",
    "section": "",
    "text": "Association rule mining (ARM) is a data analysis technique for uncovering relationships among variables in large datasets. Essentially, it operates on the principle of identifying frequent patterns, which are combinations of items that appear together in the dataset with regularity.\nThe method revolves around a key data mining algorithm known as the Frequent Pattern Mining (FPM) algorithm. FPM’s core objective is to reveal correlations and associations between items that may not be immediately obvious. This technique surfaces patterns that indicate if the presence of one set of items is related to the presence of another set.\nThese patterns are typically expressed through association rules, a form of rule-based machine learning. Association rules are useful not only for uncovering anomalies and regularities within data but also for a variety of other applications. These applications range widely, from analysis in retail and marketing, such as basket analysis and cross-marketing strategies, to more technical realms like software bug tracking.\nThe discovery of frequent itemsets using algorithms like Apriori is integral to many data mining initiatives. These itemsets form the foundation for more complex analysis, such as sequence discovery, interesting pattern recognition, and, most notably, the mining of association rules.\nA practical example of association rules in action is their use in understanding consumer behavior in retail settings. Such rules might reveal the frequency and conditions under which certain products are purchased together, offering valuable insights for marketing strategies and inventory management."
  },
  {
    "objectID": "ARM/arm.html#importing-libraries",
    "href": "ARM/arm.html#importing-libraries",
    "title": "Association Rule Mining",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder"
  },
  {
    "objectID": "ARM/arm.html#reading-in-data",
    "href": "ARM/arm.html#reading-in-data",
    "title": "Association Rule Mining",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nmerged_data = pd.read_csv('../data/Final_Data.csv')\n\n\nmerged_data['Movement_binned'] = pd.cut(merged_data['Movement'], bins=[-float('inf'), 0, float('inf')], labels=['Down', 'Up'])\n\nmerged_data['Sentiment'] = merged_data['TRAIN'].map({1: 'Positive', 0: 'Neutral', -1: 'Negative'})\n\ntransactions = merged_data.groupby('Date')[['Movement_binned', 'Sentiment']].agg(lambda x: x.tolist())\n\nte = TransactionEncoder()\nte_ary = te.fit(transactions).transform(transactions)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\n\nfrequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n\nprint(rules)\n\n       antecedents                     consequents  antecedent support  \\\n0              (M)                             (_)                0.04   \n1              (_)                             (M)                0.04   \n2              (M)                             (b)                0.04   \n3              (b)                             (M)                0.04   \n4              (M)                             (d)                0.04   \n...            ...                             ...                 ...   \n173469         (o)  (e, n, d, t, v, i, M, _, m, b)                0.04   \n173470         (M)  (e, n, d, t, v, i, o, _, m, b)                0.04   \n173471         (_)  (e, n, d, t, v, i, o, M, m, b)                0.04   \n173472         (m)  (e, n, d, t, v, i, o, M, _, b)                0.08   \n173473         (b)  (e, n, d, t, v, i, o, M, _, m)                0.04   \n\n        consequent support  support  confidence  lift  leverage  conviction  \\\n0                     0.04     0.04         1.0  25.0    0.0384         inf   \n1                     0.04     0.04         1.0  25.0    0.0384         inf   \n2                     0.04     0.04         1.0  25.0    0.0384         inf   \n3                     0.04     0.04         1.0  25.0    0.0384         inf   \n4                     0.04     0.04         1.0  25.0    0.0384         inf   \n...                    ...      ...         ...   ...       ...         ...   \n173469                0.04     0.04         1.0  25.0    0.0384         inf   \n173470                0.04     0.04         1.0  25.0    0.0384         inf   \n173471                0.04     0.04         1.0  25.0    0.0384         inf   \n173472                0.04     0.04         0.5  12.5    0.0368        1.92   \n173473                0.04     0.04         1.0  25.0    0.0384         inf   \n\n        zhangs_metric  \n0                 1.0  \n1                 1.0  \n2                 1.0  \n3                 1.0  \n4                 1.0  \n...               ...  \n173469            1.0  \n173470            1.0  \n173471            1.0  \n173472            1.0  \n173473            1.0  \n\n[173474 rows x 10 columns]\n\n\n\nAs we can see, the dataset gathered from my project does not really suit ARM as it does not have categorical variables. Stock values and Text are not the target features which apriori expects. Therefore this technique does not work correctly."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made for DSAN 5000 Lab Assignment 1"
  },
  {
    "objectID": "DataG/datagather.html",
    "href": "DataG/datagather.html",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport pandas as pd\nfrom datetime import datetime, date\nimport time\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nimport os\nimport yfinance as yf\nimport praw\n\n\nmsft = yf.Ticker(\"BBY.F\")\ndata = yf.download(\"BBY.F\", start=\"2022-06-01\", end=\"2022-12-30\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nprint(data.head())\n\ndata.to_csv('BBBY_data.csv')\n\n             Open   High    Low  Close  Adj Close  Volume\nDate                                                     \n2022-06-01  8.061  8.061  7.730  7.730      7.730     380\n2022-06-02  7.797  7.811  7.797  7.811      7.811      10\n2022-06-03  7.826  7.874  7.774  7.774      7.774     450\n2022-06-06  7.539  7.539  7.539  7.539      7.539       0\n2022-06-07  7.504  7.504  7.504  7.504      7.504       0\n\n\n\nclient_id = 'hEeZW8o7s3F8rxCEIxOeXg'\nsecret_key = 'Mgzc-xjP4I78y9vHMVSezyzolmdejA'\n\nreddit1 = praw.Reddit(\n    client_id= 'hEeZW8o7s3F8rxCEIxOeXg',\n    client_secret= 'Mgzc-xjP4I78y9vHMVSezyzolmdejA',\n    password = \"pokemons\",\n    user_agent=\"test-script\",\n    username=\"dhhheeee\",\n)\nprint(reddit.user.me())\n\ndhhheeee\n\n\n\nq='BBBY' #query\nsub='WallStreetBets' #subreddit\nsort = \"top\" \nlimit = 15000\n\n\ntop_posts = reddit.subreddit(sub).search(q, sort='hot', limit=limit)\n\nprint(top_posts)\n\n&lt;praw.models.listing.generator.ListingGenerator object at 0x13f6cfcd0&gt;\n\n\n\ntotal_posts = list()\n\n\nfor post in top_posts:\n # print(vars(post)) # print all properties\n    time.sleep(1)\n    Title=post.title,\n    Score = post.score,\n    Number_Of_Comments = post.num_comments,\n    Publish_Date = post.created,\n    Link = post.permalink,\n    data_set = {\"Title\":Title[0],\"Score\":Score[0],   \"Number_Of_Comments\":Number_Of_Comments[0],\"Publish_Date\":Publish_Date[0],\"Link\":'https://www.reddit.com'+Link[0]}\n    total_posts.append(data_set)\n\n\ndf = pd.DataFrame(total_posts)\ndf.to_csv('data.csv', sep=',', index=False)\n\n\nlibrary(quantmod)\n\nLoading required package: xts\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\nbby_df &lt;- getSymbols('BBY.f', src='yahoo', auto.assign=FALSE)\n\nWarning message:\n\"BBY.f contains missing values. Some functions will not work if objects contain missing values in the middle of the series. Consider using na.omit(), na.approx(), na.fill(), etc to remove or replace them.\"\n\n\n\nchartSeries(bby_df, name=\"BBBY\", subset=\"last 24 months\", theme=chartTheme(\"white\"))\n\n\nwrite.csv(bby_df, file = \"wallstreetbets_stock.csv\", row.names = FALSE)"
  },
  {
    "objectID": "DecisionTrees/decision.html",
    "href": "DecisionTrees/decision.html",
    "title": "Decision Trees",
    "section": "",
    "text": "In this page, I will be applying Random Forest Regressor to my Stock data and Random Forest Classifier to my Text Data."
  },
  {
    "objectID": "DecisionTrees/decision.html#introduction",
    "href": "DecisionTrees/decision.html#introduction",
    "title": "Decision Trees",
    "section": "",
    "text": "In this page, I will be applying Random Forest Regressor to my Stock data and Random Forest Classifier to my Text Data."
  },
  {
    "objectID": "DecisionTrees/decision.html#objective",
    "href": "DecisionTrees/decision.html#objective",
    "title": "Decision Trees",
    "section": "Objective:",
    "text": "Objective:\n\nTo check if random forest classifier can be used to label text data partially or completely if we can manually label the data. I have chosen to skip manually labeling the data and label it using VaderSentiment Library. I initially wanted to use Hugging Face API with RoBERTa which is basically BERT to label text data, but due to time constraints, I chose to go with VaderSentiment Library.\nTo check if we can use Random Forest Regressor to accurately predict stock data."
  },
  {
    "objectID": "DecisionTrees/decision.html#random-forest-and-decision-trees",
    "href": "DecisionTrees/decision.html#random-forest-and-decision-trees",
    "title": "Decision Trees",
    "section": "Random Forest and Decision Trees:",
    "text": "Random Forest and Decision Trees:\n\nA Random Forest model is essentially a collection of decision trees working together. Each decision tree in a Random Forest operates like a flowchart composed of a series of questions. For example, consider deciding whether to go surfing. The tree might begin with a question like “Is the weather sunny?”. Based on your answer, it leads you down a path of further queries: “Are the waves high enough?” or “Is the tide favorable?”. Each query branches out, guiding you toward a final verdict represented at the end of the path or the ‘leaf node’. If your situation meets the criteria of a question, you proceed along the ‘Yes’ branch; if not, you take the other path. Collectively, these decision trees in a Random Forest model contribute to a more refined and accurate prediction or decision than any single tree could make on its own.\nA Random forest model does not make any assumptions about the underlying distribution of your data, and can implicitly handle collinearity in features, because if you have two highly similar features, the information gain from splitting on one of the features will also use up the predictive power of the other feature. This makes Random Forest one of the most robust models to use in the real world.\nRandom forest classifier usually handles the missing values and maintains the accuracy of a larger proportion of data"
  },
  {
    "objectID": "DecisionTrees/decision.html#importing-libraries",
    "href": "DecisionTrees/decision.html#importing-libraries",
    "title": "Decision Trees",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport scipy.stats as stats\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.tree import plot_tree"
  },
  {
    "objectID": "DecisionTrees/decision.html#reading-in-stock-data",
    "href": "DecisionTrees/decision.html#reading-in-stock-data",
    "title": "Decision Trees",
    "section": "Reading in Stock Data",
    "text": "Reading in Stock Data\n\n\nCode\n#Reading in Stock Data\n\nstock_data = pd.read_csv('../data/bbbyopen.csv')"
  },
  {
    "objectID": "DecisionTrees/decision.html#exploring-the-data",
    "href": "DecisionTrees/decision.html#exploring-the-data",
    "title": "Decision Trees",
    "section": "Exploring the data",
    "text": "Exploring the data\n\n\nCode\n# Looking at the data\n\nstock_data.head()\n\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\n\n\n0\n2022-08-01 00:00:00-04:00\n4.94\n5.77\n4.86\n5.77\n5.77\n11455300\n\n\n1\n2022-08-02 00:00:00-04:00\n5.78\n6.53\n5.64\n5.79\n5.79\n19063200\n\n\n2\n2022-08-03 00:00:00-04:00\n5.75\n6.14\n5.58\n6.07\n6.07\n13848000\n\n\n3\n2022-08-04 00:00:00-04:00\n6.06\n6.49\n6.00\n6.15\n6.15\n9060800\n\n\n4\n2022-08-05 00:00:00-04:00\n6.66\n8.29\n6.52\n8.16\n8.16\n52776900"
  },
  {
    "objectID": "DecisionTrees/decision.html#calculating-the-rmse-values-for-random-forest-regressor",
    "href": "DecisionTrees/decision.html#calculating-the-rmse-values-for-random-forest-regressor",
    "title": "Decision Trees",
    "section": "Calculating the RMSE Values for Random Forest Regressor",
    "text": "Calculating the RMSE Values for Random Forest Regressor\n\n\nCode\nstock_data['Date'] = pd.to_datetime(stock_data['Date'])\nstock_data['Year'] = stock_data['Date'].dt.year\nstock_data['Month'] = stock_data['Date'].dt.month\nstock_data['Day'] = stock_data['Date'].dt.day\n\nstock_data = stock_data.drop('Date', axis=1)\n\nX = stock_data.drop('Open', axis=1) \ny = stock_data['Open']\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the RandomForestRegressor\nrandom_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model on the training data\nrandom_forest_model.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = random_forest_model.predict(X_test)\n\n# Calculate the root mean squared error\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nrmse\n\n\n3.1777676763549887"
  },
  {
    "objectID": "DecisionTrees/decision.html#looking-at-stock-data-again",
    "href": "DecisionTrees/decision.html#looking-at-stock-data-again",
    "title": "Decision Trees",
    "section": "Looking at Stock Data again",
    "text": "Looking at Stock Data again\n\n\nCode\nstock_data.head()\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\nYear\nMonth\nDay\n\n\n\n\n0\n4.94\n5.77\n4.86\n5.77\n5.77\n11455300\n2022\n8\n1\n\n\n1\n5.78\n6.53\n5.64\n5.79\n5.79\n19063200\n2022\n8\n2\n\n\n2\n5.75\n6.14\n5.58\n6.07\n6.07\n13848000\n2022\n8\n3\n\n\n3\n6.06\n6.49\n6.00\n6.15\n6.15\n9060800\n2022\n8\n4\n\n\n4\n6.66\n8.29\n6.52\n8.16\n8.16\n52776900\n2022\n8\n5"
  },
  {
    "objectID": "DecisionTrees/decision.html#plotting-the-actual-vs-predicted-values",
    "href": "DecisionTrees/decision.html#plotting-the-actual-vs-predicted-values",
    "title": "Decision Trees",
    "section": "Plotting the Actual vs Predicted values",
    "text": "Plotting the Actual vs Predicted values\n\n\nCode\nstock_data = pd.read_csv('../data/bbbyopen.csv')\n\n# Convert the 'Date' column to datetime\nstock_data['Date'] = pd.to_datetime(stock_data['Date'])\n\n# Extracting the indices of the test set\ntest_indices = X_test.index\n\n# Retrieving the corresponding dates from the original dataset using test set indices\ntest_dates = stock_data.iloc[test_indices]['Date']\n\n# Creating a DataFrame for plotting with dates\ncomparison_df_with_dates = pd.DataFrame({'Date': test_dates, 'Actual': y_test, 'Predicted': y_pred})\n\n# Sorting the DataFrame based on date for proper chronological plotting\ncomparison_df_with_dates = comparison_df_with_dates.sort_values('Date')\n\n# Plotting the actual vs predicted values\nplt.figure(figsize=(15, 7))\nplt.plot(comparison_df_with_dates['Date'], comparison_df_with_dates['Actual'], label='Actual Open Price', color='blue', marker='o')\nplt.plot(comparison_df_with_dates['Date'], comparison_df_with_dates['Predicted'], label='Predicted Open Price', color='red', marker='x')\nplt.title('Actual vs Predicted Stock Open Prices Over Time')\nplt.xlabel('Date')\nplt.ylabel('Open Price')\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "DecisionTrees/decision.html#random-forest-for-text-classification.",
    "href": "DecisionTrees/decision.html#random-forest-for-text-classification.",
    "title": "Decision Trees",
    "section": "Random Forest for Text Classification.",
    "text": "Random Forest for Text Classification.\n\nReading in Data\n\n\nCode\n# Reading in Text Data from Github\n\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')"
  },
  {
    "objectID": "DecisionTrees/decision.html#looking-at-text-data",
    "href": "DecisionTrees/decision.html#looking-at-text-data",
    "title": "Decision Trees",
    "section": "Looking at Text Data",
    "text": "Looking at Text Data\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntitle\ntimestamp\nTRAIN\n\n\n\n\n0\nThoughts on BBBY\n8/4/2022 3:09\n0.0\n\n\n1\nDraftkings earnings Tommorow. Penn beat earnin...\n8/4/2022 17:43\n0.0\n\n\n2\nBBBY and the middle game\n8/4/2022 20:46\n1.0\n\n\n3\n$BBBY 180 contracts & 1,200 shares. YOLO\n8/5/2022 4:33\n1.0\n\n\n4\nBBBY 83,378 shares , average 11.57 , LFG !!!\n8/5/2022 9:22\n1.0"
  },
  {
    "objectID": "DecisionTrees/decision.html#preprosessing-text-data",
    "href": "DecisionTrees/decision.html#preprosessing-text-data",
    "title": "Decision Trees",
    "section": "Preprosessing Text Data",
    "text": "Preprosessing Text Data\n\n\nCode\n#Preprocessing text again\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n\n\n\nCode\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n    return text\n\n\n\n\nCode\nimport nltk\nnltk.download('wordnet')\n\n\n[nltk_data] Downloading package wordnet to /Users/dheeraj/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\nCode\ndf['title'] = df['title'].apply(preprocess_text)"
  },
  {
    "objectID": "DecisionTrees/decision.html#using-vader-to-auto-label-the-title",
    "href": "DecisionTrees/decision.html#using-vader-to-auto-label-the-title",
    "title": "Decision Trees",
    "section": "Using Vader to auto label the Title",
    "text": "Using Vader to auto label the Title\n\n\nCode\nsentimentScores = []\ndef sentiment_scores(sentence):\n\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n\n    if sentiment_dict['compound'] &gt;= 0.05:\n      sentimentScores.append(1)\n    elif sentiment_dict['compound'] &lt; -0.05:\n      sentimentScores.append(-1)\n    else:\n      sentimentScores.append(0)"
  },
  {
    "objectID": "DecisionTrees/decision.html#printing-sentiment-scores-for-each-row",
    "href": "DecisionTrees/decision.html#printing-sentiment-scores-for-each-row",
    "title": "Decision Trees",
    "section": "Printing Sentiment Scores for each row",
    "text": "Printing Sentiment Scores for each row\n\nHere, 1 Means Positive and -1 Means Negative and 0 means Neutral\n\n\n\nCode\n\na = df['title'].tolist()\nfor i in a:\n    sentiment_scores(i)\nprint(sentimentScores)\n\n\n[0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, -1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 0, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, -1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 1, 1, 0, -1, 0, 1, 1, -1, 1, 1, 0, 0, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, -1, 1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, -1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, -1, 1, 1, 0, 1, 0, 1, 1, 0, -1, 1, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, -1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 1, -1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, -1, 0, -1, 0, 1, 0, 1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 1, 1, -1, 0, -1, 0, 1, 1, 0, 1, 0, 0, -1, 1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, -1, 1, 1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, -1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, -1, 0, 1, -1, 1, 1, 0, 0, -1, -1, -1, 0, 0, 0, 1, 0, 1, 0, 0, -1, -1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, -1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, -1, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, -1, 0, -1, 1, 0, 1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 0, -1, -1, 0, -1, 0, 0, -1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, -1, 1, 1, 1, -1, -1, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 1, 1, -1, 0, 0, 0, 1, 1, 1, 1, -1, 0, -1, 0, -1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 0, 1, 1, -1, 0, -1, 0, 1, -1, -1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 1, 0, -1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, -1, 1, 0, 0, 0, -1, 1, -1, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, 1, 1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, -1, -1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 1, 1, 1, 1, 0, -1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, -1, 1, -1, 1, 0, 0, 0, 1, 0, 0, -1, 1, 0, 0, 1, 1, 0, 0, 1, -1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, -1, 1, 0, 1, 0, -1, 0, 1, -1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, -1, -1, 1, -1, 0, 1, 1, 1, 0, -1, 1, 1, -1, 1, 1, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, -1, 1, -1, 1, 0, -1, 1, 0, 1, -1, 0, 0, 0, -1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, -1, 1, -1, -1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 1, 1, 0, 0, -1, 0, 0, -1, 0, -1, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, -1, -1, -1, 1, 0, 0, 1, 1, 1, 0, 0, -1, 1, -1, 1, 1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, -1, 1, -1, -1, 0, 1, -1, 1, 1, 1, 0, 1, 1, -1, 0, 0, 0, 1, -1, 1, -1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 1, 0, -1, -1, 0, -1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, 0, 1, 0, 1, -1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 1, 1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, 1, -1, 0, 1, 1, -1, 0, 0, 0, -1, -1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 1, 1, 1, -1, -1, 1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, -1, 0, 0, -1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 0, -1, -1, 0, 0, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 1, -1, 1, -1, 1, 1, 0, 0, -1, 1, 0, 0, 0, 1, -1, 0, 0, 1, 1, -1, 0, -1, 1, 0, 0, 0, 1, 1, -1, -1, 1, 1, -1, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, -1, -1, 0, 1, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 1, 1, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 1, -1, -1, 0, 0, -1, 1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 1, 0, 1, 0, -1, -1, 0, 1, 1, 0, 0, -1, 0, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, -1, 0, -1, 1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 1, 0, -1, 0, 0, 1, 1, 0, 0, 0, -1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, -1, -1, 1, 0, 1, 0, -1, -1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 1, 1, -1, 1, 1, 0, 1, 0, 0, 0, -1, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, 0, -1, -1, 1, 1, -1, 0, 1, -1, 0, -1, 0, 1, 1, 1, 0, 0, 1, 0, -1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, -1, 1, -1, 1, 1, 1, -1, 0, 0, -1, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, -1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, -1, -1, 1, 1, 1, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, -1, -1, 1, 1, 0, -1, -1, 1, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, -1, -1, 1, -1, 0, 0, 0, 0, -1, 1, 0, -1, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, -1, -1, 1, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 0, 1, 1, 1, 0, -1, 1, -1, 0, 0, 1, 0, -1, 0, 0, 0, 1, -1, 1, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 1, 1, 0, 0, 1, 0, 0, 1, 0, -1, 1, 0, -1, -1, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, -1, 1, 0, -1, -1, 0, -1, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 1, 1, 1, -1, 1, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, -1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, -1, 0, 0, -1, -1, 0, 0, 1, 0, 0, 1, 1, -1, 1, 1, 0, 0, 0, -1, 0, 1, 0, 0, 1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, -1, 0, -1, 1, 0, 1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, -1, 1, 0, 0, -1, 1, 0, -1, 0, 1, 0, 1, 1, 1, 1, -1, 0, -1, 1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, -1, 0, 0, -1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, -1, 0, 0, 1, 0, 1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, 0, -1, 1, -1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 1, 1, -1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, -1, 1, -1, 1, 1, 1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 0]\n\n\n\n\nCode\ndf['TRAIN'] = pd.Series(sentimentScores)\n\n\n\n\nCode\ndf.isna().sum()\n\n\ntitle        0\ntimestamp    0\nTRAIN        0\ndtype: int64"
  },
  {
    "objectID": "DecisionTrees/decision.html#label-distribution-for-text-data",
    "href": "DecisionTrees/decision.html#label-distribution-for-text-data",
    "title": "Decision Trees",
    "section": "Label Distribution for Text Data",
    "text": "Label Distribution for Text Data\n\n\nCode\n# Calculate the distribution of class labels\nlabel_distribution = df['TRAIN'].value_counts()\n\n# Calculate the percentage distribution\npercentage_distribution = df['TRAIN'].value_counts(normalize=True) * 100\n\n# Print the distributions\nprint(\"Label Distribution:\\n\", label_distribution)\nprint(\"\\nPercentage Distribution:\\n\", percentage_distribution)\n\n\nLabel Distribution:\n TRAIN\n 0    1567\n 1     913\n-1     468\nName: count, dtype: int64\n\nPercentage Distribution:\n TRAIN\n 0    53.154681\n 1    30.970149\n-1    15.875170\nName: proportion, dtype: float64"
  },
  {
    "objectID": "DecisionTrees/decision.html#using-tf-idf-as-a-base-for-random-forest-with-text-data",
    "href": "DecisionTrees/decision.html#using-tf-idf-as-a-base-for-random-forest-with-text-data",
    "title": "Decision Trees",
    "section": "Using TF-IDF as a base for Random Forest with Text Data",
    "text": "Using TF-IDF as a base for Random Forest with Text Data\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import squareform\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(df['title'])\ny = df['TRAIN']"
  },
  {
    "objectID": "DecisionTrees/decision.html#checking-for-na-values",
    "href": "DecisionTrees/decision.html#checking-for-na-values",
    "title": "Decision Trees",
    "section": "Checking for NA Values",
    "text": "Checking for NA Values\n\n\nCode\ndf.isna().sum()\n\n\ntitle        0\ntimestamp    0\nTRAIN        0\ndtype: int64"
  },
  {
    "objectID": "DecisionTrees/decision.html#training-random-forest-model-with-initial-parameters",
    "href": "DecisionTrees/decision.html#training-random-forest-model-with-initial-parameters",
    "title": "Decision Trees",
    "section": "Training Random Forest Model with initial parameters",
    "text": "Training Random Forest Model with initial parameters\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1337)\n\n# Train RandomForest Classifier\nmodel = RandomForestClassifier(random_state=42069)\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(len(y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n885\n              precision    recall  f1-score   support\n\n          -1       0.78      0.32      0.45       131\n           0       0.76      0.99      0.86       476\n           1       0.87      0.67      0.76       278\n\n    accuracy                           0.79       885\n   macro avg       0.80      0.66      0.69       885\nweighted avg       0.80      0.79      0.77       885"
  },
  {
    "objectID": "DecisionTrees/decision.html#plotting-multiple-decision-trees-from-the-trained-model",
    "href": "DecisionTrees/decision.html#plotting-multiple-decision-trees-from-the-trained-model",
    "title": "Decision Trees",
    "section": "Plotting multiple Decision Trees from the trained model",
    "text": "Plotting multiple Decision Trees from the trained model\n\n\nCode\n# Plotting multiple decision trees from the RandomForest\n\nfor i in range(5):\n    plt.figure(figsize=(30, 40))\n    plot_tree(model.estimators_[i], filled=True, feature_names=vectorizer.get_feature_names_out(), max_depth=15)\n    plt.title('Decision Tree '+ str(i+1) + ' from the RandomForest Classifier') \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Predictions for the confusion matrix\ny_pred = model.predict(X_test)\n\n# Creating and plotting the confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()"
  },
  {
    "objectID": "DecisionTrees/decision.html#making-a-dataframe-with-predicted-value-and-actual-value-of-the-test-set.",
    "href": "DecisionTrees/decision.html#making-a-dataframe-with-predicted-value-and-actual-value-of-the-test-set.",
    "title": "Decision Trees",
    "section": "Making a Dataframe, with Predicted Value and Actual Value of the test set.",
    "text": "Making a Dataframe, with Predicted Value and Actual Value of the test set.\n\nP.S This is an extra step done to just understand if the model is actually working as expected.\n\n\n\nCode\ndf_new = pd.DataFrame({'Predicted Value':y_pred, 'Actual Value':y_test})\ndf_new\n\n\n\n\n\n\n\n\n\nPredicted Value\nActual Value\n\n\n\n\n188\n1\n1\n\n\n2208\n0\n0\n\n\n1967\n0\n1\n\n\n1269\n1\n1\n\n\n813\n1\n1\n\n\n...\n...\n...\n\n\n1175\n0\n0\n\n\n1512\n1\n1\n\n\n2115\n0\n0\n\n\n2003\n0\n0\n\n\n239\n0\n0\n\n\n\n\n885 rows × 2 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [None, 10, 20, 30],  \n    'min_samples_split': [2, 5, 10],  \n    'min_samples_leaf': [1, 2, 4]  \n}\n\nrf = RandomForestClassifier(random_state=42049)\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best parameters found: \", grid_search.best_params_)\n\nbest_rf = grid_search.best_estimator_\n\ny_pred = best_rf.predict(X_test)\n\n\nFitting 3 folds for each of 108 candidates, totalling 324 fits\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.1s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.3s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.3s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.5s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   2.5s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   2.7s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   2.3s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   2.2s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   2.4s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.1s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   2.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.9s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.4s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   2.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   2.6s\n[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   2.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.4s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.3s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.6s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.5s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.6s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.7s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.6s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.6s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.6s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.0s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.8s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.8s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.8s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.9s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.0s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.7s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.7s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.8s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.7s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.3s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\nBest parameters found:  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n\n\n\n\nCode\nmodel = RandomForestClassifier(max_depth= None, n_estimators = 200, min_samples_leaf= 1, min_samples_split= 10)\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(len(y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n885\n              precision    recall  f1-score   support\n\n          -1       0.77      0.31      0.44       131\n           0       0.76      0.99      0.86       476\n           1       0.87      0.67      0.76       278\n\n    accuracy                           0.78       885\n   macro avg       0.80      0.65      0.68       885\nweighted avg       0.79      0.78      0.76       885\n\n\n\n\n\nCode\nplt.figure(figsize=(30, 40))\nplot_tree(model.estimators_[1], filled=True, feature_names=vectorizer.get_feature_names_out(), max_depth= 15)\nplt.title('Decision Tree '+ str(1) + ' from the RandomForest Classifier') \n\nplt.show()\n\n\n\n\n\n\n\nCode\ny_pred = model.predict(X_test)\n\n# Creating and plotting the confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()"
  },
  {
    "objectID": "DecisionTrees/decision.html#conclusions",
    "href": "DecisionTrees/decision.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions:",
    "text": "Conclusions:\n\nFrom the confusion matrix, we can see that Random Forest was able to accurately predict almost all of the data labelled by VaderSentiment. An interesting take on this would be to label the text with some form of BERT, or Roberta (Huggingface) and run the experiment again. However, that could be done as an additional improvement in the future. From the Plots of Decision Trees, we can see that the max depth of the tree should be more than 15, which makes the Decision Tree really big, and difficult to see in the website. But due to the complexity of the data, I have decided to limit the maximum depth of the tree to 15 to vizualize it better.\nGoing to record Data, we can see that Random Forest Regressor was able to predict the text data almost exactly except for the squeeze which we are studying. A more interesting take on this would be to try correlating the text data with the stock data to see if text data could be a better predictor of the stock data during the squeeze which is the main aim of this project.\n\n\n\nCode\nstock_data = pd.read_csv('../data/bbbyopen.csv')\n\n\n\n\nCode\nstock_data['Movement'] = stock_data['Close'] - stock_data['Open']\n\n\n\n\nCode\nstock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.date\n\n# Calculate the 'Movement' column as the difference between 'Close' and 'Open'\nstock_data['Movement'] = stock_data['Close'] - stock_data['Open']\n\n# Merge the datasets on the date\nmerged_data = pd.merge(stock_data, df, left_on='Date', right_on='timestamp')\n\n\n\n\nCode\nmerged_data\n\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\nMovement\ntitle\ntimestamp\nTRAIN\n\n\n\n\n0\n2022-08-04\n6.06\n6.49\n6.00\n6.15\n6.15\n9060800\n0.09\nthought bbby\n2022-08-04\n0\n\n\n1\n2022-08-04\n6.06\n6.49\n6.00\n6.15\n6.15\n9060800\n0.09\ndraftkings earnings tommorow penn beat earning...\n2022-08-04\n1\n\n\n2\n2022-08-04\n6.06\n6.49\n6.00\n6.15\n6.15\n9060800\n0.09\nbbby middle game\n2022-08-04\n0\n\n\n3\n2022-08-05\n6.66\n8.29\n6.52\n8.16\n8.16\n52776900\n1.50\nbbby 180 contract 1200 share yolo\n2022-08-05\n1\n\n\n4\n2022-08-05\n6.66\n8.29\n6.52\n8.16\n8.16\n52776900\n1.50\nbbby 83378 share average 1157 lfg\n2022-08-05\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2545\n2022-09-07\n6.90\n8.28\n6.84\n7.91\n7.91\n42453100\n1.01\n20 year old sold 110 million worth bbby interv...\n2022-09-07\n1\n\n\n2546\n2022-09-08\n7.61\n8.26\n7.55\n8.24\n8.24\n22252300\n0.63\ncramer watermelon would make cannibal bbby star\n2022-09-08\n0\n\n\n2547\n2022-09-08\n7.61\n8.26\n7.55\n8.24\n8.24\n22252300\n0.63\nbbbys market maker mm place website find marke...\n2022-09-08\n0\n\n\n2548\n2022-09-08\n7.61\n8.26\n7.55\n8.24\n8.24\n22252300\n0.63\nbbbymegaend\n2022-09-08\n0\n\n\n2549\n2022-09-08\n7.61\n8.26\n7.55\n8.24\n8.24\n22252300\n0.63\nnew mortgage program available bbby shareholder\n2022-09-08\n0\n\n\n\n\n2550 rows × 11 columns\n\n\n\n\n\nCode\n# Group by 'Date' and calculate the mean sentiment for each day\ndaily_sentiment = merged_data.groupby('Date')['TRAIN'].mean().reset_index()\n\n\n\n\nCode\n# Create a cumulative sentiment column\ndaily_sentiment['Cumulative_Sentiment'] = daily_sentiment['TRAIN'].cumsum()\n\n\n\n\nCode\n# Merge the cumulative sentiment with stock movement data\ndaily_data = pd.merge(stock_data, daily_sentiment, on='Date')\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Sort the data by date for chronological plotting\ndaily_data.sort_values('Date', inplace=True)\n\n# Create the line graph\nplt.figure(figsize=(12, 6))\n\n# Plotting stock movement\nplt.plot(daily_data['Date'], daily_data['Movement'], label='Stock Price Movement', color='blue', marker='o')\n\n# Plotting cumulative daily sentiment\nplt.plot(daily_data['Date'], daily_data['Cumulative_Sentiment'], label='Cumulative Daily Sentiment', color='green', marker='x')\n\nplt.title('Stock Price Movement and Cumulative Daily Sentiment Over Time')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.legend()\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nCode\nmerged_data.to_csv('../data/Final_Data.csv')"
  },
  {
    "objectID": "Conclusions/conclusion.html",
    "href": "Conclusions/conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "Conclusion\n\n\n\n\n\nUpon visual examination of the comparative line graph depicting stock price movements alongside the cumulative daily sentiment, I was able to conclude the following:\n\nThe cumulative sentiment over the observed period presents an ascending trajectory, suggesting a progressive build-up of positive sentiment.\nThe stock price movement demonstrates marked volatility, characterized by pronounced rises and declines within the timeframe in question. This pattern of fluctuation does not appear to mirror the more gradual trend exhibited by the sentiment line.\nA direct and consistent correlation between sentiment and stock price movement is not readily apparent from the graphical representation. Instances where a surge in positive sentiment coincides with an uptick in stock prices are sporadic and do not establish a predictable pattern.\nThe graph does not convincingly display a lag effect where sentiment shifts could be seen as preceding or predicting stock price movements.\nThe independent nature of the fluctuations observed in both sentiment and stock price suggests that while sentiment may exert some influence on stock prices, it is not the predominant factor. It is plausible that stock price movements are subject to a broader spectrum of influences beyond the scope of market sentiment as captured by the dataset.\nThe absence of a clear, direct relationship between sentiment and stock prices in the dataset implies that additional variables and external factors are likely at play in determining stock price dynamics. These may include economic indicators, corporate financial performance, geopolitical events, and broader market trends.\nGiven the complexity of financial markets and the multitude of factors affecting stock prices, a more granular approach to analysis is warranted. This approach should encompass a broader dataset, potentially including trading volumes, financial news analysis, and economic indicators. Additionally, further statistical examination, such as correlation or regression analysis, may be employed to quantify the relationship between sentiment and stock price movements more precisely.\n\nIn conclusion, the preliminary analysis does not substantiate a robust, standalone impact of market sentiment on stock prices within the examined period. While sentiment is an integral aspect of market dynamics, its role as a potential predictor or influencer of stock prices necessitates a more nuanced and multifaceted investigation to fully understand its impact within the intricate mosaic of market behavior. For future improvement, improving Labelling of sentiment through RoBERTa with Hugging Face API would be a very significant step to further improve this approach."
  },
  {
    "objectID": "Classification/classification.html",
    "href": "Classification/classification.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "from sklearn.metrics import roc_curve, precision_recall_curve\nfrom wordcloud import WordCloud"
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection",
    "title": "Naive Bayes Classifier",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Load your data\ndf = pd.read_csv('../data/bbbyopen.csv') \n\n# Create labels based on the condition that the closing price is higher than the opening price\ndf['Label'] = (df['Close'] &gt; df['Open']).astype(int)  # 1 for price increase, 0 for decrease.\n\n# Select features and labels\nX = df[['Open', 'High', 'Low', 'Volume']]  \ny = df['Label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
  },
  {
    "objectID": "Classification/classification.html#step-2-training-the-model",
    "href": "Classification/classification.html#step-2-training-the-model",
    "title": "Naive Bayes Classifier",
    "section": "Step 2: Training the Model",
    "text": "Step 2: Training the Model\n\n# Initialize Gaussian Naive Bayes\ngnb = GaussianNB()\n\n# Train the model\ngnb.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n# Make predictions\ny_pred = gnb.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Generate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(cm):\n    plt.matshow(cm, cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\nplot_confusion_matrix(cm)\n\n\n\n\nAccuracy: 0.46153846153846156\n              precision    recall  f1-score   support\n\n           0       0.50      0.86      0.63         7\n           1       0.00      0.00      0.00         6\n\n    accuracy                           0.46        13\n   macro avg       0.25      0.43      0.32        13\nweighted avg       0.27      0.46      0.34        13"
  },
  {
    "objectID": "Classification/classification.html#roc-curve-for-stock-data",
    "href": "Classification/classification.html#roc-curve-for-stock-data",
    "title": "Naive Bayes Classifier",
    "section": "ROC Curve for Stock data",
    "text": "ROC Curve for Stock data\n\nfpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(fpr, tpr, label='Naive Bayes Classifier')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Stock Data')\nplt.show()"
  },
  {
    "objectID": "Classification/classification.html#precision-recall-curve-for-stock-data",
    "href": "Classification/classification.html#precision-recall-curve-for-stock-data",
    "title": "Naive Bayes Classifier",
    "section": "Precision Recall Curve for Stock Data",
    "text": "Precision Recall Curve for Stock Data\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:,1])\nplt.plot(recall, precision, label='Naive Bayes Classifier')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Stock Data')\nplt.show()\n\n\n\n\nThe metrics provided suggest that the Naive Bayes classifier is not performing well on the testing dataset.\n\nAccuracy (0.46): This metric indicates that only about 46% of the overall predictions made by the classifier are correct. In a binary classification problem (two classes, 0 and 1), this is not much better than random guessing, which would be correct approximately 50% of the time.\nPrecision and Recall for Class 0: The precision for class 0 is 0.50, indicating that when the model predicts class 0, it is correct 50% of the time. The recall for class 0 is quite high at 0.86, showing that the model is able to identify 86% of all actual class 0 instances. However, this comes at the expense of class 1, which has neither precision nor recall scores, indicating the model did not correctly identify any class 1 instances. The F1-score for class 0, which balances precision and recall, is 0.63. This is the most robust measure among the metrics provided, but this value is still not indicative of a good predictive performance.\nClass 1 Performance: The model failed to correctly predict any instance of class 1, which is evident from the precision, recall, and F1-score all being 0. This is a clear sign that the model is heavily biased towards class 0 and is unable to generalize to class 1 instances.\nMacro Average: The macro average treats all classes equally, computing the metric independently for each class and then taking the average. The low macro average precision (0.25) and recall (0.43) further highlight the model’s poor performance across classes.\nWeighted Average: The weighted average accounts for class imbalance by weighting the metric by the number of true instances for each class. These figures (precision of 0.27 and recall of 0.46) are also low, suggesting poor performance."
  },
  {
    "objectID": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "href": "Classification/classification.html#step-1-data-preparation-feature-selection-1",
    "title": "Naive Bayes Classifier",
    "section": "Step 1: Data Preparation & Feature Selection",
    "text": "Step 1: Data Preparation & Feature Selection\n\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/dheeraj/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\n\n\n\nimport pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create a DataFrame\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')\n\n# Initialize VADER\nsia = SentimentIntensityAnalyzer()\n\n# Define a function to label sentiment based on compound score\ndef label_sentiment(row):\n    score = sia.polarity_scores(row)['compound']\n    return 'positive' if score &gt; 0.05 else 'negative' if score &lt; -0.05 else 'neutral'\n\n# Apply VADER to each title and create a new column for sentiment\ndf['sentiment'] = df['title'].apply(label_sentiment)\n\n#separating our data into training and test sets based on the 'TRAIN' column\ntrain_df = df[df['TRAIN'] == 1]\ntest_df = df[df['TRAIN'] == 0]\n\n# Preprocessing and Vectorization\nvectorizer = CountVectorizer()\n\n# Only fit the vectorizer to the training data\nX_train = vectorizer.fit_transform(train_df['title'])\ny_train = train_df['sentiment']\n\nX_test = vectorizer.transform(test_df['title'])\ny_test = test_df['sentiment'] \n\n# Train a Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()"
  },
  {
    "objectID": "Classification/classification.html#sentiment-score-distribution",
    "href": "Classification/classification.html#sentiment-score-distribution",
    "title": "Naive Bayes Classifier",
    "section": "Sentiment Score Distribution",
    "text": "Sentiment Score Distribution\n\n# Histogram of Sentiment Scores\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.histplot(df['sentiment'], bins=10, kde=False)\n\n# Give the histogram a title and labels\nplt.title('Sentiment Score Distribution')\nplt.xlabel('Sentiment Score')\nplt.ylabel('Number of Text Entries')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "Classification/classification.html#frequency-of-sentiment-labels",
    "href": "Classification/classification.html#frequency-of-sentiment-labels",
    "title": "Naive Bayes Classifier",
    "section": "Frequency of Sentiment Labels",
    "text": "Frequency of Sentiment Labels\n\nsns.countplot(x='sentiment', data=df, palette='viridis')\n\nplt.title('Frequency of Sentiment Labels')\nplt.xlabel('Sentiment Label')\nplt.ylabel('Frequency')\n\nplt.show()\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_94862/3616295182.py:1: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(x='sentiment', data=df, palette='viridis')"
  },
  {
    "objectID": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "href": "Classification/classification.html#word-cloud-of-most-frequent-words-in-positive-class",
    "title": "Naive Bayes Classifier",
    "section": "Word Cloud of Most Frequent Words in Positive Class",
    "text": "Word Cloud of Most Frequent Words in Positive Class\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n# Here, we are filtering for positive sentiment\npositive_titles = df[df['sentiment'] == 'positive']['title']\n\n# Concatenate all the positive titles into one single string\ntext = \" \".join(title for title in positive_titles)\n\n# Generate the word cloud object\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')  # Turn off the axis\nplt.show()"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Python:\nPython is a versatile and widely-used programming language in the field of data science due to its readability and the extensive ecosystem of data-centric libraries. For this project, the following libraries would be employed:\n\nPandas: This library is fundamental for data manipulation and analysis. It provides data structures and operations for manipulating numerical tables and time series. It is particularly well-suited for handling structured data, like the stock and text datasets we are dealing with.\nNumPy: Often used in conjunction with Pandas, NumPy adds support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. It’s essential for numerical computations.\nMatplotlib: This plotting library offers a MATLAB-like interface and is excellent for creating static, interactive, and animated visualizations in Python. It’s highly customizable and can be used to make the histograms and scatter plots that are often required in EDA.\nSeaborn: Built on top of Matplotlib, Seaborn is a statistical graphics library that provides a high-level interface for drawing attractive and informative statistical graphics. It’s particularly good for creating more complex charts like heatmaps for correlation analysis with less code.\nScikit-learn: Although primarily known for its machine learning algorithms, scikit-learn also includes various tools for data preprocessing, model selection, and evaluation metrics that are vital for the classification tasks we are undertaking.\nNatural Language Toolkit (NLTK): For text data, NLTK is a powerful Python library that provides tools for working with human language data (text). It’s useful for tasks such as sentiment analysis, tokenization, and stopwords removal.\nVADER (from NLTK): A lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media. It’s beneficial for labeling our text data based on sentiment.\nWordCloud: While not as analytical as other tools, WordCloud can be used to visualize the most prominent words in the text data, which might give insights into the overall sentiment or topics of discussion.\nJupyter Notebook or JupyterLab: For an interactive computing environment where we can combine code execution, rich text, visualizations, and other media, Jupyter Notebook or JupyterLab will be used. It’s particularly helpful for sharing the analysis process in a readable format that includes both code and commentary.\n\n\n\n\n# Import seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf1 = pd.read_csv('../data/bbbyopen.csv', index_col='Date', parse_dates=True)\n\n# Basic statistics\nprint(\"Basic Statistical Details:\")\nprint(df1.describe())\n\n\nBasic Statistical Details:\n            Open       High        Low      Close  Adj Close        Volume\ncount  43.000000  43.000000  43.000000  43.000000  43.000000  4.300000e+01\nmean    9.613721  10.715116   8.859302   9.659070   9.659070  6.085124e+07\nstd     3.960792   5.189946   3.215582   3.811286   3.811286  7.683056e+07\nmin     4.940000   5.770000   4.860000   5.770000   5.770000  7.908500e+06\n25%     7.095000   7.735000   6.800000   7.180000   7.180000  1.401785e+07\n50%     8.740000   9.120000   8.350000   8.760000   8.760000  3.142170e+07\n75%    10.800000  11.740000   9.895000  10.570000  10.570000  7.872405e+07\nmax    26.940001  30.000000  22.500000  23.080000  23.080000  3.953199e+08\n\n\n\n\n\n\n# Compute the correlation matrix\ncorr_matrix = df.corr()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n# Scatter plot of two variables with strong correlation\nsns.scatterplot(data=df, x='Open', y='Volume')\nplt.show()"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Import seaborn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf1 = pd.read_csv('../data/bbbyopen.csv', index_col='Date', parse_dates=True)\n\n# Basic statistics\nprint(\"Basic Statistical Details:\")\nprint(df1.describe())\n\n\nBasic Statistical Details:\n            Open       High        Low      Close  Adj Close        Volume\ncount  43.000000  43.000000  43.000000  43.000000  43.000000  4.300000e+01\nmean    9.613721  10.715116   8.859302   9.659070   9.659070  6.085124e+07\nstd     3.960792   5.189946   3.215582   3.811286   3.811286  7.683056e+07\nmin     4.940000   5.770000   4.860000   5.770000   5.770000  7.908500e+06\n25%     7.095000   7.735000   6.800000   7.180000   7.180000  1.401785e+07\n50%     8.740000   9.120000   8.350000   8.760000   8.760000  3.142170e+07\n75%    10.800000  11.740000   9.895000  10.570000  10.570000  7.872405e+07\nmax    26.940001  30.000000  22.500000  23.080000  23.080000  3.953199e+08"
  },
  {
    "objectID": "eda/eda.html#correlation-matrix",
    "href": "eda/eda.html#correlation-matrix",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# Compute the correlation matrix\ncorr_matrix = df.corr()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n# Scatter plot of two variables with strong correlation\nsns.scatterplot(data=df, x='Open', y='Volume')\nplt.show()"
  },
  {
    "objectID": "eda/eda.html#time-series-analysis-of-stock-data",
    "href": "eda/eda.html#time-series-analysis-of-stock-data",
    "title": "Exploratory Data Analysis",
    "section": "Time Series analysis of Stock Data",
    "text": "Time Series analysis of Stock Data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf1 = pd.read_csv('../data/bbbyopen.csv')\nprint(df1.columns)\n# Parse the 'Date' column to datetime while considering the timezone (-04:00)\ndf1['Date'] = pd.to_datetime(df1['Date'], utc=True)\n\n\ndf1.set_index('Date', inplace=True)\n\n# Convert the index to the desired timezone (the data has -04:00 which corresponds to US Eastern time)\ndf1.index = df1.index.tz_convert('US/Eastern')\n\n# Plotting 'Close' price\nplt.figure(figsize=(14, 7))\nplt.plot(df1.index, df1['Close'], label='Close Price', marker='o')\n\n# Adding titles and labels\nplt.title('Stock Close Price Time Series')\nplt.xlabel('Date')\nplt.ylabel('Close Price in $')\nplt.legend()\n\n# Improving design\nplt.grid(True)\nplt.tight_layout()\n\n# Rotate date labels\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\nIndex(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
  },
  {
    "objectID": "eda/eda.html#date-range",
    "href": "eda/eda.html#date-range",
    "title": "Exploratory Data Analysis",
    "section": "Date Range:",
    "text": "Date Range:\nThe data ranges from August 1, 2022, to September 29, 2022, giving us nearly two months of trading data to analyze."
  },
  {
    "objectID": "eda/eda.html#price-movement",
    "href": "eda/eda.html#price-movement",
    "title": "Exploratory Data Analysis",
    "section": "Price Movement:",
    "text": "Price Movement:\n\nOpening Prices:\nThe opening prices have fluctuated significantly. The stock opened at $4.94 on August 1 and reached a high opening of $26.94 on August 17, indicating a substantial price increase within the month.\n\n\nClosing Prices:\nClosing prices saw similar volatility, starting at $5.77 and spiking up to $23.08 by August 17 before dropping down to $6.19 by the end of the period.\n\n\nHighs and Lows:\nThe stock experienced considerable intraday volatility. For instance, on August 16, the stock reached a high of $28.60 but also had a low of $15.36 within the same day.\n\n\nVolume:\nTrading volume varied widely, with particularly high volumes coinciding with large price movements. For example, on August 8 and 16, where the closing prices were $11.41 and $20.65, the volumes were 122,664,300 and 395,319,900 respectively, which are significantly higher than the rest of the dates."
  },
  {
    "objectID": "eda/eda.html#findings",
    "href": "eda/eda.html#findings",
    "title": "Exploratory Data Analysis",
    "section": "Findings",
    "text": "Findings\n\nKey Terms\n\nStock Focus: “GME” and “BBBY” are the most prominent terms, indicating a high volume of discussion centered around GameStop and Bed Bath & Beyond stocks.\nTrading Vocabulary: Common trading terms such as “buy,” “sell,” “share,” and “stock” are prevalent.\n\n\n\nInvestor Behavior and Sentiment\n\nCommunity Lingo: Words like “moon,” “YOLO,” “ape,” and “HODL” suggest a community-driven, speculative approach to trading, aiming for substantial short-term gains.\nMarket Tactics: Discussions are rich with strategic terms like “short,” “squeeze,” “bullish,” and “put,” showing a deep engagement with market maneuvering.\n\n\n\nTemporal Focus\n\nImmediate Action: The prominence of “today,” “tomorrow,” and “now” highlights a focus on immediate trading decisions rather than long-term investment strategies.\n\n\n\nEmotional and Personal Investment\n\nRisk and Strategy: The term “YOLO” alongside “holding” illustrates a blend of risk-taking and strategic retention in investment decisions.\n\n\n\nInfluential Figures\n\nNotable Personalities: The mention of “Ryan Cohen” points to discussions that likely reflect the impact of key individuals in the trading community."
  },
  {
    "objectID": "eda/eda.html#discussion",
    "href": "eda/eda.html#discussion",
    "title": "Exploratory Data Analysis",
    "section": "Discussion",
    "text": "Discussion\nThe word cloud illustrates that the discussions are not just about trading strategies but also about community dynamics, with a marked tendency towards speculative trading. This is emblematic of a shift in investment culture, where individual investors collectively and informally share insights, strategies, and reactions to market events on digital platforms. The analysis provides a snapshot of the zeitgeist of modern retail investing, characterized by immediacy, community identity, and the influence of social media on market movements."
  },
  {
    "objectID": "eda/eda.html#conclusion",
    "href": "eda/eda.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the word cloud analysis of the text data from trading discussions has unveiled a vivid picture of the previous trading landscapes. It highlights the interplay between collective sentiment and individual decision-making, where community vernacular and strategic discourse converge. This underscores the impact of social platforms on investment practices and the evolving nature of the stock market in the digital age."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In my project, I’m delving into my dataset gathered from Reddit, which comprises scraped titles from posts on the Reddit WallStreetBets (WSB) subreddit which are related to the topic Bed Bath and Beyond. My goal with the clustering analysis is to sift through these titles for patterns and themes that reveal the collective mood and recurring topics of discussion. I want to see which sentiments dominate the conversation and understand the nature of the discourse—whether it’s generally bullish or bearish, and if it’s suggesting specific trading actions. This endeavor is not just about mapping out the frequency of mentions but also about gauging the sway this online community has over market sentiment and stock movements.\nBy applying clustering to the post titles, I’m looking to categorize them into clusters that might correspond to different sentiments, investment advice, or highlight the most talked-about ‘meme stocks’. I aim to ascertain the reliability of the sentiments expressed and consider the feasibility of crafting an investment strategy based on the collective insights of the WSB community. Additionally, I’m curious to compare the activity before and after the Bed Bath & Beyond incident to determine if that was a standalone event or a magnification of a pre-existing pattern within the subreddit’s interactions."
  },
  {
    "objectID": "clustering/clustering.html#kmeans-to-generate-wordcloud-for-data-in-each-cluster",
    "href": "clustering/clustering.html#kmeans-to-generate-wordcloud-for-data-in-each-cluster",
    "title": "Clustering",
    "section": "Kmeans to generate wordcloud for data in each cluster",
    "text": "Kmeans to generate wordcloud for data in each cluster\n\n\nCode\nn_clusters = 6\nmodel = KMeans(n_clusters=n_clusters, random_state=42)\nmodel.fit(text_features)\n\n# Extracting features and words for word clouds\nfeatures = vectorizer.get_feature_names_out()\nclusters = model.labels_\n\n# Creating a dictionary to hold words for each cluster\nclustered_words = defaultdict(list)\nfor i, label in enumerate(clusters):\n    for feature, value in zip(features, text_features.toarray()[i]):\n        if value &gt; 0:  # adding only words present in the document\n            clustered_words[label].append(feature)\n\n# Generating word clouds for each cluster\nfig, axes = plt.subplots(1, n_clusters, figsize=(15, 5))\nfor i in range(n_clusters):\n    wordcloud = WordCloud(width=400, height=400, background_color='white').generate(' '.join(clustered_words[i]))\n    axes[i].imshow(wordcloud, interpolation='bilinear')\n    axes[i].axis('off')\n    axes[i].set_title(f'Cluster {i+1}')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html#final-thoughts-and-conclusion",
    "href": "clustering/clustering.html#final-thoughts-and-conclusion",
    "title": "Clustering",
    "section": "Final Thoughts and Conclusion:",
    "text": "Final Thoughts and Conclusion:\n\nThe effectiveness and preference of a clustering method can be context-dependent. If the data has outliers or non-spherical clusters, DBSCAN may perform better.\nKMeans is often preferred for its simplicity and speed, especially on large datasets where the number of clusters is known or can be estimated.\nHierarchical clustering provides a comprehensive view of potential clusterings based on distance and can be ideal for exploratory data analysis when the number of clusters is not known.\nThe above dendogram represents that the text data is too large to fit into proper clusters. This implies that the text data maybe too complex or has too many dimensions to cluster correctly. There is a lot of overlap between the clusters and the text data which is probably due to the fact that this method is not optimal for text data.\n\n\nReferences:\n[1][https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/#:~:text=DBSCAN%2Density%2DBased%20Spatial%20Clustering,and%20classifying%20outliers%20as%20noise.]\n\n[2][https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html]\n\n[3][https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html]\n\n[4][https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/]\n\n[5][https://en.wikipedia.org/wiki/Hierarchical_clustering]"
  },
  {
    "objectID": "lab-2.1.html",
    "href": "lab-2.1.html",
    "title": "Lab-2.1: Assignment",
    "section": "",
    "text": "Gathering text data with an API\nIMPORTANT: The lab shown here (on the website) is just an HTML version, included for reference. To download the assignment, please navigate to the Labs tab in the Share-point dropdown menu in the website’s navigation bar. The relevant assignment can be determined from the folder’s name, click on the three dots & select download to get the assignment.\nNOTE: It is recommended that you complete this .ipynb file in VS-code.\nSubmission:"
  },
  {
    "objectID": "lab-2.1.html#assignment-1",
    "href": "lab-2.1.html#assignment-1",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-1:",
    "text": "Assignment-1:\n\nRead over the News-API, Wikipedia API, and Google Scholar API sections in the lab-demonstration section, if you have not done so already.\n\nhttps://jfh.georgetown.domains/dsan5000/\n\nGet an API key for the News-API: see following link\nSubmission: Insert your API key below\n\n\nAPI_KEY='e5fb283476a34988a2f2358c58a08d0e'"
  },
  {
    "objectID": "lab-2.1.html#assignment-2",
    "href": "lab-2.1.html#assignment-2",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-2:",
    "text": "Assignment-2:\n\nUse the provided News-API code as a starting point\nSelect THREE random topics (e.g. Georgetown, Cats, Clouds) but choose whatever you like\nQuery the API to pull text data and store the results in three different dictionaries\nExtract the title and description text and store for later processing (up to you how you do this)\nClean the text as needed\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Lambda handler function for API calls.\ndef lambdaHandler(topic):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n    total_requests=2\n    verbose=True\n    TOPIC = topic\n    URLpost = {'apiKey': API_KEY,\n                'q': '+'+TOPIC,\n                'sortBy': 'relevancy',\n                'totalRequests': 1}\n\n    # print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url)\n    # print(response.url);  \n    response = response.json()\n\n    #print(json.dumps(response, indent=2))\n\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(TOPIC + '-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n    return response\n\n\n# Function to clean up strings\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n# Function to clean up data before conversion\ndef cleaner(response):\n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    total_requests=2\n    verbose=True\n    print(\"AVAILABLE KEYS:\")\n    print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            #if(verbose):\n                #print(\"----------------\")\n                #print(key)\n                #print(article[key])\n                #print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    return cleaned_data\n\n\ndef dataF(cleaned_data, topic):\n    #Function to convert a cleaned data frame into a CSV file.\n    df = pd.DataFrame(cleaned_data)\n    print(df)\n    df.to_csv(str(topic) + 'cleaned.csv' ,index_label=['index','title','description'])\n\n\nif __name__ == \"__main__\":\n    #Creating dataframes for 3 different topics and storing them in CSV files.\n    topic = [\"mango\", \"DOW\" , \"Watch\"]\n    for i in topic:\n        a = lambdaHandler(topic=i)\n        b = cleaner(response=a)\n        dataF(b,i)"
  },
  {
    "objectID": "lab-2.1.html#assignment-3",
    "href": "lab-2.1.html#assignment-3",
    "title": "Lab-2.1: Assignment",
    "section": "Assignment-3:",
    "text": "Assignment-3:\n\nUse the provided Wikipedia-API code as a starting point\nFor EACH THREE of the random topics, create a word cloud for your cleaned title and description text\n\n\nimport wikipedia\n\n\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n    # exit()\n    # Import package\n    # Define a function to plot word cloud\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\ntext='The field of machine learning is typically divided into three fundamental sub-paradigms. These include supervised learning, unsupervised learning, and reinforcement learning (RL). The discipline of reinforcement learning focuses on how intelligent agents learn to perform actions, inside a specified environment, to maximize  a cumulative reward function. Over the past several decades, there has been a push to incorporate concepts from the field of deep-learning into the agents used in RL algorithms. This has spawned the field of Deep reinforcement learning. To date, the field of deep RL has yielded stunning results in a wide range of technological applications. These include, but are not limited to, self-driving cars, autonomous game play, robotics, trading and finance, and Natural Language Processing. This course will begin with an introduction to the fundamentals of traditional, i.e. non-deep, reinforcement learning. After reviewing fundamental deep learning topics the course will transition to deep RL by incorporating artificial neural networks into the models. Topics include Markov Decision Processes, Multi-armed Bandits, Monte Carlo Methods, Temporal Difference Learning, Function Approximation, Deep Neural Networks, Actor-Critic, Deep Q-Learning, Policy Gradient Methods, and connections to Psychology and to Neuroscience.'\n\ngenerate_word_cloud(text)\n\n\n\n\n\ndef print_info(wiki_page):\n    print(\"-------------------------\")\n    print(wiki_page.title)\n    print(wiki_page.url)\n    print(wiki_page.sections)\n\n    if(verbose):\n        print(wiki_page.sections)\n        print(wiki_page.categories)\n        print(wiki_page.html)\n        print(wiki_page.images)\n        print(wiki_page.content)\n        print(wikipedia.summary(wiki_page.title, auto_suggest=False))\n        print(wiki_page.references)\n        print(wiki_page.links[0],len(page.links))\n\n#--------------------------\n# LOOP OVER COUNTRY AND TOPIC \n#--------------------------\n\ndef wordcloudGen(list3, topic_list):\n    for i in list3:\n    \n        text=''\n        #--------------------------\n        # USER INPUTS\n        #--------------------------\n        for topic in topic_list:\n            topic=topic+' in '+ i \n            print(\"topic = \",topic)\n            max_num_pages=2     #max num pages returned by wiki search\n            verbose=False\n\n            #--------------------------\n            #SEARCH FOR RELEVANT PAGES \n            #--------------------------\n            titles=wikipedia.search(topic,results=max_num_pages)\n            print(\"TITLES=\",titles)\n            \n            #--------------------------\n            #LOOP OVER TITLES\n            #--------------------------\n            num_files=0\n            for title in titles:\n                try:\n                    page = wikipedia.page(title, auto_suggest=False)\n                    print_info(page)\n                    text = text + page.content\n                    num_files+=1\n                except:\n                    print(\"SOMETHING WENT WRONG:\", title);  \n\n    generate_word_cloud(text)\n\n\nif __name__ == \"__main__\":\n    list1=['computer science']\n    list2=['Car']\n    list3=['Nike']\n    topic_list = ['System Architecture', 'Microprocessor', 'Software Engineering']\n    topic_list2 = ['horsepower', 'weight']\n    topic_list3 = ['shoes', 'basketball']\n    wordcloudGen(list1, topic_list)\n    wordcloudGen(list2, topic_list2)\n    wordcloudGen(list3, topic_list3)\n    \n\ntopic =  System Architecture in computer science\nTITLES= ['Computer architecture', 'Outline of computer science']\n-------------------------\nComputer architecture\nhttps://en.wikipedia.org/wiki/Computer_architecture\n[]\n-------------------------\nOutline of computer science\nhttps://en.wikipedia.org/wiki/Outline_of_computer_science\n[]\ntopic =  Microprocessor in computer science\nTITLES= ['Microcomputer', 'Microprocessor']\n-------------------------\nMicrocomputer\nhttps://en.wikipedia.org/wiki/Microcomputer\n[]\n-------------------------\nMicroprocessor\nhttps://en.wikipedia.org/wiki/Microprocessor\n[]\ntopic =  Software Engineering in computer science\nTITLES= ['Computer science and engineering', 'Software engineering']\n-------------------------\nComputer science and engineering\nhttps://en.wikipedia.org/wiki/Computer_science_and_engineering\n[]\n-------------------------\nSoftware engineering\nhttps://en.wikipedia.org/wiki/Software_engineering\n[]\ntopic =  horsepower in Car\nTITLES= ['List of production cars by power output', 'Horsepower']\n-------------------------\nList of production cars by power output\nhttps://en.wikipedia.org/wiki/List_of_production_cars_by_power_output\n[]\n-------------------------\nHorsepower\nhttps://en.wikipedia.org/wiki/Horsepower\n[]\ntopic =  weight in Car\nTITLES= ['Vehicle weight', 'Weight']\n-------------------------\nVehicle weight\nhttps://en.wikipedia.org/wiki/Vehicle_weight\n[]\n-------------------------\nWeight\nhttps://en.wikipedia.org/wiki/Weight\n[]\ntopic =  shoes in Nike\nTITLES= ['Nike, Inc.', 'Air Force (shoe)']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nAir Force (shoe)\nhttps://en.wikipedia.org/wiki/Air_Force_(shoe)\n[]\ntopic =  basketball in Nike\nTITLES= ['Nike, Inc.', 'Nike Elite Youth Basketball League']\n-------------------------\nNike, Inc.\nhttps://en.wikipedia.org/wiki/Nike,_Inc.\n[]\n-------------------------\nNike Elite Youth Basketball League\nhttps://en.wikipedia.org/wiki/Nike_Elite_Youth_Basketball_League\n[]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dheeraj Oruganty",
    "section": "",
    "text": "Dheeraj Oruganty, DO343 is a student at Georgetown University studying Masters in Data Science and Analytics. When not innovating on data platforms, Dheeraj enjoys spending time travelling and going out to explore places\n\nEducation\nGeorgetown University, DC | Washington, DC\nMS in Data Science and Analytics | Aug 2023 - May 2025\n\n\nExperience\nMachine Learning Intern - Cluzters.ai | Aug 2021 to Feb 2022"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is an introduction to my project on Reddit extracting data from r/WallStreetBets\nThe authors of this paper delve into the role of the social media platform Reddit in the GameStop (GME) share rally of early 2021. Specifically, they examine how discussions on the r/WallStreetBets subreddit influenced the price dynamics of GameStop. To do this, they create a custom sentiment analysis dictionary for Reddit users using the Valence Aware Dictionary and Sentiment Reasoner (VADER) sentiment analysis package and analyze a massive dataset comprising 10.8 million comments. Their analysis uncovers significant relationships between Reddit sentiments and GameStop returns at various time intervals (1-, 5-, 10-, and 30-minutes), contributing valuable insights to the expanding body of research on “meme stocks” and the impact of discussions within investment forums on intraday stock price movements. Anand and Pathak (2022)\nReddit’s WallStreetBets (WSB) community has come to prominence in light of its notable role in affecting the stock prices of what are now referred to as meme stocks. Yet very little is known about the reliability of the highly speculative investment advice disseminated on WSB. This paper analyses WSB data spanning from January 2019 to April 2021 in order to assess how successful an investment strategy relying on the community’s recommendations could have been. We detect buy and sell advice and identify the community’s most popular stocks, based on which we define a WSB portfolio. Our evaluation shows that this portfolio has grown approx. 200% over the last three years and approx. 480% over the last year, significantly outperforming the S&P500. The average short-term accuracy of buy and sell signals, in contrast, is not found to be significantly better than randomly or equally distributed buy decisions within the same time frame. However, we present a technique for estimating whether posts are proactive as opposed to reactive and show that by focusing on a subset of more promising buy signals, a trader could have made investments yielding higher returns than the broader market or the strategy of trusting all posted buy signals. Lastly, the analysis is also conducted specifically for the period before 2021 in order to factor out the effects of the GameStop hype of January 2021 - the results confirm the conclusions and suggest that the 2021 hype merely amplified pre-existing characteristics. Buz and Melo (2021)\n\nWhat sentiments and discussions on Reddit’s investing subreddits, like r/WallStreetBets, have influenced Bed Bath & Beyond stock recently?\nHow active is the Bed Bath & Beyond subreddit, and what are its members saying about the company’s stock?\nAre there any popular Reddit posts or comments that have affected the sentiment and trading activity around Bed Bath & Beyond stock?\nHave there been any notable Reddit-driven movements in Bed Bath & Beyond stock, similar to what was seen with GameStop in early 2021?\nHow do Reddit users perceive the future prospects and potential catalysts for Bed Bath & Beyond’s stock?\nAre there any memes, trends, or inside jokes related to Bed Bath & Beyond stock on Reddit that might be influencing investor sentiment?\nWhat is the sentiment analysis of Reddit comments and discussions regarding Bed Bath & Beyond stock?\nHow do Reddit discussions align with or differ from traditional financial analysis and forecasts for Bed Bath & Beyond stock?\nAre there any Reddit users or influencers known for their insights or influence on Bed Bath & Beyond stock, and what are their views?\nWhat are the pros and cons of considering Reddit sentiment and discussions when making investment decisions related to Bed Bath & Beyond stock?\n\n\n\n\n\nReferences\n\nAnand, Abhinav, and Jalaj Pathak. 2022. “The Role of Reddit in the GameStop Short Squeeze.” Economics Letters 211: 110249.\n\n\nBuz, Tolga, and Gerard de Melo. 2021. “Should You Take Investment Advice from WallStreetBets? A Data-Driven Approach.” arXiv Preprint arXiv:2105.02728."
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom io import StringIO\ndf = pd.read_csv('data.csv')\nprint(df.head())\n\ndf.columns\n\n       title score       id  \\\n0  Wash sale     0   txhzpp   \n1    Comment     1  i3lsqo4   \n2    Comment     1  i3lrk43   \n3    Comment     1  i3lmzkh   \n4    Comment     1  i3lmz13   \n\n                                                 url  comms_num       created  \\\n0  https://www.reddit.com/r/wallstreetbets/commen...        8.0  1.649236e+09   \n1                                                NaN        0.0  1.649238e+09   \n2                                                NaN        0.0  1.649237e+09   \n3                                                NaN        0.0  1.649233e+09   \n4                                                NaN        0.0  1.649233e+09   \n\n                                                body            timestamp  \n0  Hi guys I bought uvxy beginning of the year in...  2022-04-06 09:14:16  \n1                          Yes December 30 I sold it  2022-04-06 09:32:46  \n2         ![img](emote|t5_2th52|4260) Just need help  2022-04-06 09:15:56  \n3                                   ??? Alright bro.  2022-04-06 08:10:16  \n4  Elon is with more than some of the S&P 500 com...  2022-04-06 08:10:03  \n\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/3347210500.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('data.csv')\n\n\nIndex(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n       'timestamp'],\n      dtype='object')\nfiltered_df = df[df['title'].str.contains('BBBY', case=False, na=False)]\n\nfiltered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\nfiltered_df1 = filtered_df[filtered_df['comms_num'] &gt; 5]\n\nfiltered_df1\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2740493494.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['comms_num'] = filtered_df['comms_num'].astype(int)\n\n\n\n\n\n\n\n\n\ntitle\nscore\nid\nurl\ncomms_num\ncreated\nbody\ntimestamp\n\n\n\n\n242\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n268\ntrrxgt\nhttps://www.reddit.com/gallery/trrxgt\n33\n1.648594e+09\nNaN\n2022-03-29 22:44:07\n\n\n285\nRyan Cohen knows what he is doing with $BBBY a...\n2221\ntr1ln5\nhttps://www.reddit.com/gallery/tr1ln5\n129\n1.648565e+09\nNaN\n2022-03-29 14:36:27\n\n\n343\nJust another gains porn post to add to the mas...\n147\ntqkb6l\nhttps://i.redd.it/cpuz69lq37q81.png\n6\n1.648504e+09\nNaN\n2022-03-28 21:49:59\n\n\n411\nBBBY 250k yolo - bend em over Ryan Cohen\n1086\ntqcqb6\nhttps://i.redd.it/gall3rdve5q81.jpg\n68\n1.648484e+09\nNaN\n2022-03-28 16:08:17\n\n\n5546\nBBBY- Why I’m bullish and you should be too.\n72\ntwjqla\nhttps://www.reddit.com/r/wallstreetbets/commen...\n75\n1.649124e+09\nDisclaimer- I have no fking clue what I’m doin...\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n421595\nBBBY can keep you warm at night\n254\n131dzxb\nhttps://i.redd.it/qfnqwecj4jwa1.png\n16\n1.682646e+09\nNaN\n2023-04-28 01:38:48\n\n\n424457\nEven in Canada the $BBBY party is over\n1135\n133txnt\nhttps://i.redd.it/4civ1miit2xa1.jpg\n115\n1.682866e+09\nNaN\n2023-04-30 14:51:39\n\n\n441966\nBBBYQ hodlers with nuked accounts watching the...\n2636\n13gmycg\nhttps://v.redd.it/koph30sg9oza1\n365\n1.683998e+09\nNaN\n2023-05-13 17:06:51\n\n\n458074\nYou can now buy all the store equipment and fi...\n430\n13v91g2\nhttps://www.reddit.com/gallery/13v91g2\n94\n1.685400e+09\nNaN\n2023-05-29 22:47:16\n\n\n542326\nBBBY is producing Degenerates\n321\n163krrj\nhttps://i.redd.it/w39r8pxdhukb1.png\n110\n1.693226e+09\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 8 columns\nfiltered_df = filtered_df1[['body', 'title', 'timestamp']]\n\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n242\nNaN\nBBBY ~25K GAINZ...BEEN A GOOD YEAR\n2022-03-29 22:44:07\n\n\n285\nNaN\nRyan Cohen knows what he is doing with $BBBY a...\n2022-03-29 14:36:27\n\n\n343\nNaN\nJust another gains porn post to add to the mas...\n2022-03-28 21:49:59\n\n\n411\nNaN\nBBBY 250k yolo - bend em over Ryan Cohen\n2022-03-28 16:08:17\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n...\n...\n...\n...\n\n\n421595\nNaN\nBBBY can keep you warm at night\n2023-04-28 01:38:48\n\n\n424457\nNaN\nEven in Canada the $BBBY party is over\n2023-04-30 14:51:39\n\n\n441966\nNaN\nBBBYQ hodlers with nuked accounts watching the...\n2023-05-13 17:06:51\n\n\n458074\nNaN\nYou can now buy all the store equipment and fi...\n2023-05-29 22:47:16\n\n\n542326\nhttps://www.cnbc.com/2023/08/28/bed-bath-beyon...\nBBBY is producing Degenerates\n2023-08-28 12:28:37\n\n\n\n\n2689 rows × 3 columns\n# Ensure that 'timestamp' column is in datetime format\nfiltered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\n\n# Filter the data based on the date range\nstart_date = '2022-04-01'\nend_date = '2023-01-10'\nfiltered_df = filtered_df[(filtered_df['timestamp'] &gt;= start_date) & (filtered_df['timestamp'] &lt;= end_date)]\n\n/var/folders/mt/q564ct8s021gvdq22vy117hm0000gn/T/ipykernel_3020/2011051996.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['timestamp'] = pd.to_datetime(filtered_df['timestamp'])\nfiltered_df\n\n\n\n\n\n\n\n\nbody\ntitle\ntimestamp\n\n\n\n\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n5656\nRyan Cohen tweets in code again? Is BbBY about...\nBBBY about to rocket?\n2022-04-04 04:37:21\n\n\n10641\nI’m just getting started in options and I feel...\nBBBY Short Term Estimates\n2022-04-08 18:20:48\n\n\n13821\nNaN\nShorting $BBBY let's see what happens tomorrow\n2022-04-12 23:13:21\n\n\n14750\nNaN\nBBBY YOLO - after turning 4k to 50k on BBBY sh...\n2022-04-14 23:06:44\n\n\n...\n...\n...\n...\n\n\n213329\nWent YOLO on $111k of BBBY 2024 corporate bond...\nBBBY 2024 BONDS YOLO\n2022-10-14 03:52:26\n\n\n218886\nNaN\nPsychology of Market cycles in perspective of ...\n2022-10-18 18:30:30\n\n\n244418\nI was reading his page on Wikipedia and stumbl...\nExcerpts from the class action suit against Ry...\n2022-11-09 03:58:50\n\n\n259696\nThis was his first video interview in a couple...\nRyan Cohen speaks about selling his BBBY stake 🍉\n2022-11-20 21:33:14\n\n\n296299\nNaN\nI heard you guys like gain porn (Citadel made ...\n2023-01-06 19:25:11\n\n\n\n\n2631 rows × 3 columns\nfiltered_df.to_csv('CleanedData.csv')\ndf = pd.read_csv('CleanedData.csv')\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nbody\ntitle\ntimestamp\n\n\n\n\n0\n5546\nDisclaimer- I have no fking clue what I’m doin...\nBBBY- Why I’m bullish and you should be too.\n2022-04-05 01:54:42\n\n\n1\n5656\nRyan Cohen tweets in code again? Is BbBY about...\nBBBY about to rocket?\n2022-04-04 04:37:21\n\n\n2\n10641\nI’m just getting started in options and I feel...\nBBBY Short Term Estimates\n2022-04-08 18:20:48\n\n\n3\n13821\nNaN\nShorting $BBBY let's see what happens tomorrow\n2022-04-12 23:13:21\n\n\n4\n14750\nNaN\nBBBY YOLO - after turning 4k to 50k on BBBY sh...\n2022-04-14 23:06:44"
  },
  {
    "objectID": "data/data.html#cleaned-data-is-uploaded-to-github.",
    "href": "data/data.html#cleaned-data-is-uploaded-to-github.",
    "title": "Data Cleaning",
    "section": "Cleaned data, is uploaded to github.",
    "text": "Cleaned data, is uploaded to github.\n\nhttps://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv"
  },
  {
    "objectID": "data/data.html#cleaning-data-further-removing-emojis-stopwords-and-punctuation-from-the-text.",
    "href": "data/data.html#cleaning-data-further-removing-emojis-stopwords-and-punctuation-from-the-text.",
    "title": "Data Cleaning",
    "section": "Cleaning Data further, removing emojis, stopwords and punctuation from the text.",
    "text": "Cleaning Data further, removing emojis, stopwords and punctuation from the text.\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/dheerajoruganty/RedditSentimentAnalysisWSB/main/title.csv')\n\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n    return text\n\n\ndf['title'] = df['title'].apply(preprocess_text)\n\n\nExporting to CSV after cleaning\n\ndf.to_csv('CleanedData_Final.csv')"
  }
]